1. Question
An application is currently in production that makes calls to an AWS RDS Instance. The application consists of a reporting module and a transactional system. During high load times, the response time for the application used to get very high. This was being attributed to the number of queries being fired against the database system. Which of the following can be used to resolve the response time for the application?

 Place a cloudfront distribution in front of the database
 Enable Read Replica’s for the database		-Correct
 Move the database to DynamoDB
 Enable Multi-AZ for the database
Unattempted
Answer – B
The AWS Documentation mentions the following
You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads.
Option A is incorrect since normally cloudfront distribution are placed in front of the front tier of the application
Option C is incorrect since changing the entire architecture is not the ideal approach
Option D is incorrect since this is used for fault tolerant scenarios for the database
For more information on Read Replicas, please refer to the below URLhttps://aws.amazon.com/rds/details/read-replicas/



2. Question
An application is currently in production that makes calls to an AWS RDS Instance. The database has recently been facing performance problems. It has been noticed that the same queries are putting a strain on the database. Which of the following can be used to resolve the issue?

 Place a cloudfront distribution in front of the database
 Enable Multi-AZ for the database
 Place an SQS queue in front of the database
 Place an ElastiCache in front of the database		-Correct
Unattempted
Answer – D
The AWS Documentation mentions the following
Proposed solution: an in-memory cache based on Amazon ElastiCache
Because the issue involves latency to the backend database, we propose an in-memory cache based on Amazon ElastiCache to reduce network latency and to offload the database pressure. This solution dramatically reduces the data retrieval latency. It also scales request volume considerably, because Amazon ElastiCache can deliver extremely high request rates, measured at over 20 million per second. The following diagram shows the proposed architecture.
Option A is incorrect since normally cloudfront distribution are placed in front of the front tier of the application
Option B is incorrect since this is used for fault tolerant scenarios for the database
Option C is incorrect since this is used for queuing of messages
For more information on reducing latency’s for hybrid architectures , please refer to the below URL
https://aws.amazon.com/blogs/database/latency-reduction-of-hybrid-architectures-with-amazon-elasticache/



3. Question
You’re a developer at a company that needs to deploy an application using Elastic Beanstalk. There is a requirement to place a healthcheck.config file for the environment. In which of the following location should this config file be placed to ensure it is part of the elastic beanstalk environment?

 In the application root folder
 In the config folder
 In the packages folder
 In the .ebextensions folder		-Correct
Unattempted
Answer – D
The AWS Documentation mentions the following
Elastic Beanstalk supports two methods of saving configuration option settings. Configuration files in YAML or JSON format can be included in your application’s source code in a directory named .ebextensions and deployed as part of your application source bundle. You create and manage configuration files locally.
All other options are incorrect because the AWS documentation specifically mentions that you need to place custom configuration files in the .ebextensions folder
For more information on the environment configuration method , please refer to the below URL
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-methods-before.html



4. Question
A Lambda function has been developed with the default settings and is using Node.js. The function makes calls to a DynamoDB table. It is estimated that the lambda function would run for 5 minutes. When the lambda function is executed, it is not adding the required rows to the DynamoDB table. What needs to be changed in order to ensure that the Lambda function works as desired?

 Ensure that the underlying programming language is changed to python
 Change the timeout for the function		-Correct
 Change the memory assigned to the function to 1 GB
 Assign an IAM user to the Lambda function
Unattempted
Answer – B
If the lambda function was created with the default settings , it would have the default timeout of 3 seconds as shown below. Since the function executes in a timespan of 300 seconds on an EC2 instance , this value needs to be changed.

Option A is incorrect since the programming language is not an issue
Option C is incorrect since there is no mention on the amount of memory required in the question
Option D is incorrect since IAM roles should be assigned to the Lambda function
For more information on configuring Lambda functions , please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html



5. Question
You are working on a POC for a new gaming application in us-east-1 region which will be using Amazon Cognito Events to execute AWS Lambda function. AWS Lambda function will issue a winning badge for a player post reaching every new level. You are getting error as “LambdaThrottledException” for certain cases when you are performing application testing with large number of players. Which of the following action needs to be implemented to resolve this error message?

 Make sure Lambda Function responds in 5 sec.
 Make sure Amazon Cognito provides all records in a dataset as input to the function.
 Retry sync operation.		-Correct
 Make sure you are updating “datasetRecords” field & not any other fields.
Unattempted
Correct Answer – C
To resolve “LambdaThrottledException” error while using Amazon Cognito Events, you need to perform retry on sync operations while writing Lambda function.
Option A is incorrect as If a Lambda Function does not respond in 5 sec, “LambdaSocketTimeoutException” error will be generated & not “LambdaThrottledException”
Option B is incorrect as this will not generate error “LambdaThrottledException”.
Option D is incorrect as If you are updating other fields, this will result in failure in updating records & not generate error “LambdaThrottledException”.
For more information on Amazon Cognito Events, refer to the following URL,
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-events.html



6. Question
An application needs to make use of a messaging system. The messages need to be processed in the order they are received and also no duplicates should be allowed. Which of the following would you use for this purpose?

 Enable FIFO on an existing SQS Standard Queue.
 Add the .fifo extension to the existing Standard SQS Queue
 Consider using SNS
 Use the FIFO SQS Queues		-Correct
Unattempted
Answer – D
This is also mentioned in the AWS Documentation
FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can’t be tolerated, for example:
Ensure that user-entered commands are executed in the right order.
Display the correct product price by sending price modifications in the right order.
Prevent a student from enrolling in a course before registering for an account.
options A and B are incorrect since an existing Standard queue can’t be changed to FIFO.
Q: Can I convert my existing standard queue to a FIFO queue?

No. You must choose the queue type when you create it. However, it is possible to move to a FIFO queue
Option C is incorrect since this is a notification service and not a queuing service
For more information on SQS FIFO Queues, please refer to the below URL
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html



7. Question
Which of the following is the right sequence of hooks that get called in AWS CodeDeploy?

 Application Stop->BeforeInstall->After Install->Application Start		-Correct
 BeforeInstall->After Install-> Application Stop-> Application Start
 BeforeInstall->After Install->Validate Service-> Application Start
 BeforeInstall->Application Stop-> Validate Service-> Application Start
Unattempted
Answer – A
This is also mentioned in the AWS Documentation

Because of the order of events given in the AWS Documentation , all other options are invalid.

For more information on the hooks order , please refer to the below URL
https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order



8. Question
As a developer, you have created a Lambda function that is used to work with a bucket in Amazon S3. The Lambda function is not working as expected. You need to debug the issue and understand what’s the underlying issue. How can you accomplish this in an easily understandable way?

 Use AWS Cloudwatch metrics
 Use AWS CloudWatch logs		-Correct
 Set the Lambda function debugging level to verbose
 Use AWS Cloudtrail logs
Unattempted
Answer – B
This is also mentioned in the AWS Documentation
You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with Amazon CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function (/aws/lambda/).
Option A is incorrect since the metrics will only give the rate at which the function is executing , but not help debug the actual error
Option C is incorrect since there is no such option
Option D is incorrect since this is only used for API monitoring

For more information on monitoring functions, please refer to the below URL: https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html



9. Question
You are developing a function that will be hosted in AWS Lambda. The function will be developed in .Net. There are a number of external libraries that are needed for the code to run. Which of the following is the best practise when it comes to working with external dependencies for AWS Lambda?

 Make sure that the dependencies are put in the root folder
 Selectively only include the libraries that are required		-Correct
 Make sure the libraries are installed in the beginning of the function
 Place the entire SDK dependencies in Amazon S3
Unattempted
Answer – B
This is also mentioned in the AWS Documentation
Minimize your deployment package size to its runtime necessities. This will reduce the amount of time that it takes for your deployment package to be downloaded and unpacked ahead of invocation. For functions authored in Java or .NET Core, avoid uploading the entire AWS SDK library as part of your deployment package. Instead, selectively depend on the modules which pick up components of the SDK you need
Option A is incorrect since dependencies don’t need to be in the root folder
Option C is incorrect since they can run at runtime and don’t need to be installed prior
Option D is incorrect since using the entire SDK sets is not advisable
For more information on best practises for AWS Lambda , please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html



10. Question
Your team has a Code Commit repository in your account. You need to give developers in another account access to your Code Commit repository. Which of the following is the most effective way to grant access?

 Create IAM users for each developer and provide access to the repository
 Create an IAM Group , add the IAM users and then provide access to the repository
 Create a cross account role , give the role the privileges. Provide the role ARN to the developers.		-Correct
 Enable public access for the repository.
Unattempted
Answer – C
This is also mentioned in the AWS Documentation
Configure Cross-Account Access to an AWS CodeCommit Repository
You can configure access to AWS CodeCommit repositories for IAM users and groups in another AWS account. This is often referred to as cross-account access. This section provides examples and step-by-step instructions for configuring cross-account access for a repository named MySharedDemoRepo in the US East (Ohio) Region in an AWS account (referred to as AccountA) to IAM users who belong to an IAM group named DevelopersWithCrossAccountRepositoryAccess in another AWS account (referred to as AccountB).

All other options are incorrect because all of them are not recommended practises for giving access

For more information on an example for cross account role access , please refer to the below URL
https://docs.aws.amazon.com/codecommit/latest/userguide/cross-account.html



11. Question
You have a lambda function that is processed asynchronously. You need a way to check and debug issues if the function fails? How could you accomplish this?

 Use AWS Cloudwatch metrics
 Assign a dead letter queue		-Correct
 Configure SNS notifications
 Use AWS Cloudtrail logs
Unattempted
Answer – B
This is also mentioned in the AWS Documentation

Any Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you’re unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure.

Option A is incorrect since the metrics will only give the rate at which the function is executing , but not help debug the actual error
Option C is incorrect since this will only provide notifications but not give the actual events which failed.
Option D is incorrect since this is only used for API monitoring

For more information on dead letter queues with AWS Lambda , please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/dlq.html



12. Question
You are planning to use AWS Kinesis streams for an application being developed for a company. The company policy mandates that all data is encrypted at rest. How can you accomplish this in the easiest way possible for Kinesis streams?

 Use the SDK for Kinesis to encrypt the data before being stored at rest
 Enable server-side encryption for Kinesis streams		-Correct
 Enable client-side encryption for Kinesis streams
 Use the AWS CLI to encrypt the data
Unattempted
Answer – B
The easiest way is to use the in-built server-side encryption that is available with Kinesis streams
The AWS Documentation mentions the following
Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it’s at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it’s written to the Kinesis stream storage layer, and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.
Options A and C are invalid since this would involve too much of effort for encrypting and decrypting to the streams
Option D is invalid since this is the same as encrypting the data before it reaches the stream
For more information on server-side encryption with streams , please refer to the below URL
https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html



13. Question
You are developing an application that is going to make use of Amazon Kinesis. Due to the high throughput , you decide to have multiple shards for the streams. Which of the following is TRUE when it comes to processing data across multiple shards?

 You cannot guarantee the order of data across multiple shards. It's possible only within a shard		-Correct
 Order of data is possible across all shards in a stream
 Order of data is not possible at all in Kinesis streams
 You need to use Kinesis firehose to guarantee the order of data
Unattempted
Answer – A

Kinesis Data Streams lets you order records and read and replay records in the same order to many Kinesis Data Streams applications. To enable write ordering, Kinesis Data Streams expects you to call the PutRecord API to write serially to a shard while using the sequenceNumberForOrdering parameter. Setting this parameter guarantees strictly increasing sequence numbers for puts from the same client and to the same partition key.

Option A is correct as it cannot guarantee the ordering of records across multiple shards.
Option B,C and D are incorrect because Kinesis Data Streams can order records on a single shard.
Each data record has a sequence number that is unique within its shard. Kinesis Data Streams assigns the sequence number after you write to the stream with putRecords or client.putRecord.

For more information please refer:

https://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/
https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html



14. Question
A company is planning on developing an application that is going to make use of a DynamoDB table. The structure of the table is given below
Larger image
Which of the following should be chosen as the partition key to ensure the MOST effective distribution of keys?

 Product ID
 Review ID		-Correct
 Product Name
 Production Description
Unattempted
Answer – B
The most effective one will be the Review ID since you have a uniquely generated GUID for each record.
Option A is partially correct. It can be used as the partition key , but the question asks for the MOST effective distribution of keys and that would be the Review ID
Options C and D are incorrect since it would not be a best practise to keep these as the partition keys
For more information on DynamoDB , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html
https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/



15. Question
Your company currently stores its objects in S3. The current request rate is around 11000 GET requests per second. There is now a mandate for objects to be encrypted at rest. So you enable encryption using KMS. There are now performance issues being encountered. What could be the main reason behind this?

 Amazon S3 will now throttle the requests since they are now being encrypted using KMS
 You need to also enable versioning to ensure optimal performance
 You are now exceeding the throttle limits for KMS API calls		-Correct
 You need to also enable CORS to ensure optimal performance
Unattempted
Answer – C
This is also mentioned in the AWS Documentation
You can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The limit applies to both kinds of requests.Option A is invalid since S3 will not throttle requests just because encryption is enabled.
For example, you might store data in Amazon S3 using server-side encryption with AWS KMS (SSE-KMS). Each time you upload or download an S3 object that’s encrypted with SSE-KMS, Amazon S3 makes a GenerateDataKey (for uploads) or Decrypt (for downloads) request to AWS KMS on your behalf. These requests count toward your limit, so AWS KMS throttles the requests if you exceed a combined total of 5500 (or 10,000) uploads or downloads per second of S3 objects encrypted with SSE-KMS.
Options B and D are invalid since these will not help increase performance
For more information on KMS limits improvement, please refer to the below URLhttps://docs.aws.amazon.com/kms/latest/developerguide/limits.html



16. Question
Your company is planning on using the Simple Storage service to host objects that will be accessed by users. There is a speculation that there would be roughly 6000 GET requests per second. Which of the following is the right way to use object keys for optimal performance?

 exampleawsbucket/2019-14-03-15-00-00/photo1.jpg
 exampleawsbucket/sample/232a-2019-14-03-15-00-00photo1.jpg
 exampleawsbucket/232a-2019-14-03-15-00-00/photo1.jpg		-Correct
 exampleawsbucket/sample/photo1.jpg
Unattempted
Answer – C
The AWS Documentation mentions the following on optimal performance for S3

All other options are incorrect since they are not the right ways to store object keys for optimal performance
Note:
Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. It is simple to increase your read or write performance exponentially. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.

For more details, please check below AWS Docs:
https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html



17. Question
You are working on an application which uses Amazon Cognito. Data in Amazon Cognito needs to be further analysed using Amazon Redshift. You are planning to use Kinesis streams for this purpose. Which of the following can be performed to have Amazon Cognito Events push events to Kinesis streams to get analysed data from Amazon Redshift?

 Only use an existing Kinesis Stream & create an IAM role which grants Amazon Cognito permission to publish to this existing Stream.
 
 Use an existing Kinesis Stream or Create a new Kinesis Stream & create an IAM role which grants Amazon Cognito permission to publish to Stream.		-Correct
 
 Create a new kinesis stream instead of using an existing Kinesis stream and create an IAM user with permissions to Amazon Cognito to publish to this new stream.
 
 Create a new Kinesis Stream & enable an Amazon Cognito Streams which will automatically start putting events in the selected stream.
 
Unattempted
Correct Answer – B
You can select either existing Kinesis Stream or create a new Kinesis Stream to have Amazon Cognito Streams to push all sync data to these streams. Apart from this an IAM role is required to grant Amazon Cognito to put data events to these streams.
Option A is incorrect as Both existing Kinesis Stream or a new Kinesis Stream can be used.
Option C is incorrect as an IAM role is needed here instead of an IAM user
Option D is incorrect as IAM roles should be selected which grants Amazon Cognito permission to publish to this Stream.
For more information on Amazon Cognito Streams, refer to the following URL,https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-streams.html



18. Question
You’ve developed a Lambda function and are now in the process of debugging it. You add the necessary print statements in the code to assist in the debugging. You go to Cloudwatch logs , but you see no logs for the lambda function. Which of the following could be the underlying issue for this?

 You’ve not enabled versioning for the Lambda function
 The IAM role needed for the lambda function to write the logs to Cloudwatch logs does not have necessary permissions		-Correct
 There is not enough memory assigned to the function
 There is not enough time assigned to the function
Unattempted
Answer – B
The AWS Documentation mentions the following
Note
“If your Lambda function code is executing, but you don’t see any log data being generated after several minutes, this could mean your execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs. For information about how to make sure that you have set up the execution role correctly to grant these permissions, see Manage Permissions: Using an IAM Role (Execution Role)”.

Option A is incorrect since versioning will not help in this case
Options C and D are incorrect since if these were the cases, then the function would not complete execution.
For more information on monitoring Lambda functions, please refer to the below URL:https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html



19. Question
A DynamoDB table has a Read Throughput capacity of 5 RCU. Which of the following read configuration will provide us the maximum read throughput?

 Read capacity set to 5 for 4KB reads of data at strong consistency
 Read capacity set to 5 for 4KB reads of data at eventual consistency		-Correct
 Read capacity set to 15 for 1KB reads of data at strong consistency
 Read capacity set to 5 for 1KB reads of data at eventual consistency
Unattempted
Answer – B
The calculation of throughput capacity for option B would be
Read capacity(5) * Amount of data(4) = 20.
Since its required at eventual consistency , we can double the read throughput to 20*2=40
For Option A
Read capacity(5) * Amount of data(4) = 20. Since we need strong consistency we have would get a read throughput of 20
For Option C
Read capacity(15) * Amount of data(1) = 15. Since we need strong consistency we have would get a read throughput of 15
For Option D
Read capacity(5) * Amount of data(1) = 5. Since we need eventual consistency we have would get a read throughput of 5*2=10
For more information on DynamoDB throughput , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html



20. Question
Your team is developing a solution that will make use of DynamoDB tables. Due to the nature of the application, the data is needed across a couple of regions across the world. Which of the following would help reduce the latency of requests to DynamoDB from different regions?

 Enable Multi-AZ for the DynamoDB table
 Enable global tables for DynamoDB		-Correct
 Enable Indexes for the table
 Increase the read and write throughput for the table
Unattempted
Answer – B
The AWS Documentation mentions the following
Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution. When you create a global table, you specify the AWS regions where you want the table to be available. DynamoDB performs all of the necessary tasks to create identical tables in these regions, and propagate ongoing data changes to all of them.
For more information on Global tables , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html



21. Question
Your company has a large set of data sets that need to be streamed directly into Amazon S3. Which of the following would be perfect for such a requirement?

 Kinesis Streams
 Kinesis Data Firehose		-Correct
 AWS Redshift
 AWS DynamoDB
Unattempted
Answer – B
The AWS Documentation mentions the following
Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk.
Option A is partially valid , but since the stream of data needs to go directly into S3 , Firehose can be used instead of Kinesis streams
Option C is invalid because this is used as a petabyte warehouse system
Option D is invalid because this is an AWS fully managed NoSQL database.
For more information on Kinesis Firehose , please refer to the below URL
https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html



22. Question
A company is planning on using Amazon Kinesis firehose to stream data into an S3 bucket. They need the data to be transformed first before it can be sent to the S3 bucket. Which of the following would be used for the transformation process?

 AWS SQS
 AWS Lambda		-Correct
 AWS EC2
 AWS API Gateway
Unattempted
Answer – B
The AWS Documentation mentions the following
Kinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.
Because of what the AWS Documentation mentions , all other options are invalid
For more information on Kinesis Firehose , please refer to the below URL
https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html



23. Question
Your company is hosting a static web site in S3. The code has recently been changed wherein Javascript calls are being made to the web pages in the same bucket via the FQDN. But the browser is blocking the requests. What should be done to alleviate the issue?

 Enable CORS on the bucket		-Correct
 Enable versioning on the bucket
 Enable CRR on the bucket
 Enable encryption the bucket
Unattempted
Answer – A
Option B is incorrect because this is used to prevent accidental deletion of objects in S3
Option C is incorrect because this is used for Cross region replication of objects
Option D is incorrect because this is used to encrypt objects at rest

The AWS Documentation mentions the following
Cross-Origin Resource Sharing: Use-case Scenarios
The following are example scenarios for using CORS:

Scenario 1: Suppose that you are hosting a website in an Amazon S3 bucket named website as described in Hosting a Static Website on Amazon S3. Your users load the website endpoint http://website.s3-website-us-east-1.amazonaws.com. Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket, website.s3.amazonaws.com. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross-origin requests from website.s3-website-us-east-1.amazonaws.com.
Scenario 2: Suppose that you want to host a web font from your S3 bucket. Again, browsers require a CORS check (also called a preflight check) for loading web fonts. You would configure the bucket that is hosting the web font to allow any origin to make these requests.

For more information on Cross Origin access , please refer to the below URL
https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html



24. Question
A company is storing sensitive data in their S3 bucket. The company policy states that all objects in the S3 bucket need to be encrypted at rest. Which of the following help ensure this policy is met?

 Deny permission to upload an object if the header does not include x-amz-server-side-encryption		-Correct
 Deny permission to upload an object if the header includes x-amz-server-side-encryption
 Deny permission to upload an object if the header does not include x-allow-encryption
 Deny permission to upload an object if the header includes x-allow-encryption
Unattempted
Answer – A
This is also given in the AWS Documentation

Since the documentation clearly mentions what is the requirement for encryption to upload objects , all other options are invalid.

For more information on Server-Side Encryption , please refer to the below URL
https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html



25. Question
As a developer, you have enabled server logging on an S3 bucket. You have a simple static web page with CSS pages uploaded to the bucket which is 1 MB in total size. After a duration of 2 weeks, you come back and see that the size of the bucket has increased to 50MB. Which of the following could be a reason for this?

 You have enabled CRR on the bucket as well , that is why the space is being consumed
 You have enabled Encryption on the bucket as well , that is why the space is being consumed
 Server access logs are configured to be delivered to the same bucket as the source bucket.		-Correct
 Monitoring has been enabled for the bucket
Unattempted
Answer – C
An S3 bucket with server access logging enabled can accumulate many server log objects over time. Your application might need these access logs for a specific period after creation, and after that, you might want to delete them. You can use Amazon S3 lifecycle configuration to set rules so that Amazon S3 automatically queues these objects for deletion at the end of their life.

The correct answer is C. This is normal behaviour since the logs are being delivered to the same bucket.

For more information on deleting logs files , please refer to the below URLhttps://docs.aws.amazon.com/AmazonS3/latest/dev/deleting-log-files-lifecycle.html



26. Question
Your application is developed to pick up metrics from several servers and push them off to Cloudwatch. At times , the application gets client 429 errors. Which of the following can be done from the programming side to resolve such errors?

 Use the AWS CLI instead of the SDK to push the metrics
 Ensure that all metrics have a timestamp before sending them across
 Use exponential backoff in your requests		-Correct
 Enable encryption for the requests
Unattempted
Answer – C
The main reason for such errors is that throttling is occurring when many requests are sent via API calls. The best way to mitigate this is to stagger the rate at which you make the API calls.
This is also given in the AWS Documentation
In addition to simple retries, each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and maximum number of retries are not necessarily fixed values and should be set based on the operation being performed, as well as other local factors, such as network latency.
Option A is invalid , because this accounts to the same thing. It’s basically the number of requests that is the issue.
Option B is invalid because anyway you have to add the timestamps when sending the requests
Option D is invalid because this would not help in the issue
For more information on API retries , please refer to the below URL
https://docs.aws.amazon.com/general/latest/gr/api-retries.html



27. Question
A company currently allows access to their API’s to customers via the API gateway. Currently all clients have a 6-month period to move from using the older API’s to newer versions of the API’s. The code for the API is hosted in AWS Lambda. Which of the following is the ideal strategy to employ in such a situation?

 Create another AWS Lambda version and give the link to that version to the customers.
 Create another AWS Lambda ALIAS and give the link to that version to the customers.
 Create another stage in the API gateway		-Correct
 Create a deployment package that would automatically change the link to the new Lambda version
Unattempted
Answer – C
The best way is to create a separate stage in the API gateway as maybe ‘v2’ and then customers could use both API versions. They can still slowly change their usage onto the new version in this duration.
Below is the concept of the API stage in the AWS Documentation
API stage
“A logical reference to a lifecycle state of your API (for example, ‘dev’, ‘prod’, ‘beta’, ‘v2’). API stages are identified by API ID and stage name”.

Options A and B are incorrect since access needs to be provided via the gateway
Option D is incorrect since you need to keep both versions running side by side
For more information on the API gateway , please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html



28. Question
You are using Amazon Cognito identity pools to assign authenticated SAML users a temporary access for downloading data from Amazon S3 buckets. For this you have created multiple rules for each role which gets assigned to users. Which of the following criteria is matched for evaluating these rules?

 Rules are evaluated in sequential order & rule with lower value is preferred.
 Rules are evaluated in sequential order & IAM role for first matching rule is used ,unless a standard attribute is specified to override the order.
 Rules are evaluated in sequential order & rule with higher value is preferred.
 Rules are evaluated in sequential order & IAM role for first matching rule is used ,unless a ‘CustomRoleArn” is specified to override the order.		-Correct
Unattempted
Correct Answer – D
When multiple rules are assigned, rules are evaluated in a sequential order & IAM role for first matching rule is used unless a ‘CustomRoleArn” attribute is added to modify this sequence.
Option B is incorrect as a standard attribute does not alter rule evaluation.
Option A & C are incorrect as for each rule there is no preference value.
For more information on Role-Based Access Control for AWS Cognito, refer to the following URL,
https://docs.aws.amazon.com/cognito/latest/developerguide/role-based-access-control.html



29. Question
A company has a cloudformation template that is used to create a huge list of resources. It creates a VPC, subnets , EC2 Instances , Autoscaling Groups , Load Balancers etc. Which of the following should be considered when designing such Cloudformation templates?

 Ensure to create one entire stack from the template
 Look towards breaking the templates into smaller manageable templates		-Correct
 Package the templates together and use the cloudformation deploy command
 Package the templates together and use the cloudformation package command
Unattempted
Answer – B
This recommendation is also given in the AWS Documentation
As your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate out these common components and create dedicated templates for them. That way, you can mix and match different templates but use nested stacks to create a single, unified stack. Nested stacks are stacks that create other stacks. To create nested stacks, use the AWS::CloudFormation::Stack resource in your template to reference other templates.
Option A is incorrect since this is not the recommended design practise.
Options C and D are incorrect because these are used for packaging and deployment and not for the design stages
For more information on best practises for Cloudformation , please refer to the below URL
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html



30. Question
You are developing a common lambda function that will be used across several environments such as staging, development etc. The lambda function needs to interact with each of these environments. What is the best way to develop the Lambda function?

 Create a Lambda function for each environment so that each function can point to its respective environment.
 Create one Lambda function and use environment variables for each environment to interact.		-Correct
 Create one Lambda function and create several versions for each environment.
 Create one Lambda function and create several ALIAS for each environment.
Unattempted
Answer – B
This is also mentioned in the AWS Documentation
Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK. AWS Lambda then makes these key value pairs available to your Lambda function code using standard APIs supported by the language, like process.env for Node.js functions.
Option A is incorrect since this would result in unnecessary code functions and more maintenance requirement for the functions
Options C and D are incorrect since these are not the right way to design the functions for this use case

For more information on Lambda environment variables, please refer to the below URLhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html



31. Question
As a developer you have been told to create an API gateway stage that will directly interact with DynamoDB tables. Which of the following feature of the API Gateway must be used to fulfill this requirement?

 Ensure to create an Integration request		-Correct
 Ensure to enable CORS
 Ensure to enable DAX
 Enable Binary payloads
Unattempted
Answer – A
This is also mentioned in the AWS Documentation
For example, with DynamoDB as the backend, the API developer sets up the integration request to forward the incoming method request to the chosen backend. The setup includes specifications of an appropriate DynamoDB action, required IAM role and policies, and required input data transformation. The backend returns the result to API Gateway as an integration response. To route the integration response to an appropriate method response (of a given HTTP status code) to the client, you can configure the integration response to map required response parameters from integration to method
Option B is incorrect since this is only required for cross domain requests.
Option C is incorrect since this is only required for low latency to DynamoDB tables
Option D is incorrect since this is only required is the request is not a text-based request
For more information on the developer experience for the API gateway , please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html#api-gateway-overview-developer-experience
https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/



32. Question
You have recently developed an AWS Lambda function to be used as a backend technology for an API gateway instance. You need to give the API gateway URL to a set of users for testing. What must be done before the users can test the API?

 Ensure that a deployment is created in the API gateway		-Correct
 Ensure that CORS is enabled for the API gateway
 Generate the SDK for the API
 Enable support for binary payloads
Unattempted
Answer – A
This is also mentioned in the AWS Documentation
In API Gateway, a deployment is represented by a Deployment resource. It is like an executable of an API represented by a RestApi resource. For the client to call your API, you must create a deployment and associate a stage to it. A stage is represented by a Stage resource and represents a snapshot of the API, including methods, integrations, models, mapping templates, Lambda authorizers (formerly known as custom authorizers), etc
Option B is incorrect since this is only required for cross domain requests.
Option C is incorrect since this is only required when you want to use your code to call the API gateway and there is no mention of that requirement in the question
Option D is incorrect since this is only required is the request is not a text-based request and there is no mention of the type of payload in the question
For more information on setting up deployments , please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-deployments.html



33. Question
You just developed code in AWS Lambda that makes use of recursive functions. After several invocations, you are beginning to see throttling errors in the metrics. Which of the following should be done to resolve this issue?

 Place the recursive function in a separate package
 Use versioning for the recursive function
 Avoid using recursive code altogether		-Correct
 Use the API gateway to call the recursive code.
Unattempted
Answer – C
This is also clearly mentioned in the AWS Documentation
Avoid using recursive code in your Lambda function, wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to unintended volume of function invocations and escalated costs. If you do accidentally do so, set the function concurrent execution limit to 0 immediately to throttle all invocations to the function, while you update the code..

Because of the recommendations that is mentioned in the AWS Documentation , all other options are incorrect.
For more information on the best practises for AWS Lambda , please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html



34. Question
An application is making a request to AWS STS for temporary access credentials. Below is the response being received:




AQoDYXdzEPT//////////wEXAMPLEtc764bNrC9SAPBSM22wDOk4x4HIZ8j4FZTwdQW
LWsKWHGBuFqwAeMicRXmxfpSPfIeoIYRqTflfKD8YUuwthAx7mSEI/qkPpKPi/kMcGd
QrmGdeehM4IC1NtBmUpp2wUE8phUZampKsburEDy0KPkyQDYwT7WZ0wq5VSXDvp75YU
9HFvlRd8Tx6q6fE8YQcHNVXAkiY9q6d+xo0rKwT38xVqr7ZD0u0iPPkUL64lIZbqBAz
+scqKmlzm8FDrypNC9Yjc8fPOLn9FX9KSYvKTr4rvx3iSIlTJabIQwj2ICCR/oLxBA==


wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY

2011-07-15T23:28:33.359Z
AKIAIOSFODNN7EXAMPLE


arn:aws:sts::123456789012:assumed-role/demo/lambda
ARO123EXAMPLE123:lambda
6

c6104cbe-af31-11e0-8154-cbc7ccf896c7


Which of the following is TRUE with regards to the above response?

 The SecretAccessKey can be used like Access keys to make request to resources
 The application will assume the role of arn:aws:sts::123456789012:assumed-role/demo/lambda		-Correct
 The session token will be valid for the lifetime of the application
 The Request ID can be used to make requests to access other AWS resources
Unattempted
Answer – B
Some of the aspects that get incorporated in the call to STS are
The Amazon Resource Name (ARN) of the role that the app should assume.
The duration, which specifies the duration of the temporary security credentials.
A role session name, which is a string value that you can use to identify the session. This value can be captured and logged by CloudTrail to help you distinguish between your role users during an audit.
Options A and D are invalid because you need the session token to make requests to access other AWS resources
Option C is invalid because these tokens are short lived tokens
For more information on temporary access credentials , please refer to the below URL
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html



35. Question
You have an application that is hosted on an EC2 Instance. This application is part of a custom domain http://www.demo.com The application has been changed to make calls to the API gateway. But the browser is not rendering the responses and Javascript errors are being seen in the developer console. What must be done to ensure that this issue can be resolved?

 Make the application call a Lambda function instead.
 There is an issue with the stage defined on the API gateway, hence define a new stage
 Make use of Cognito user pools
 Enable CORS for the API gateway		-Correct
Unattempted
Answer – D
This is given in the AWS Documentation
###########
When your API’s resources receive requests from a domain other than the API’s own domain, you must enable cross-origin resource sharing (CORS) for selected methods on the resource. This amounts to having your API respond to the OPTIONS preflight request with at least the following CORS-required response headers:
Access-Control-Allow-Methods
Access-Control-Allow-Headers
Access-Control-Allow-Origin
#############

Option A is invalid because you should not make the architecture change , since this is not the underlying issue.
Option B is invalid because this is the problem with CORS and not the stage itself
Option C is invalid because using Cognito user pools would just add one more unnecessary layer of authentication which is not part of the question requirement.
For more information on CORS for API gateway, please refer to the below URLhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html



36. Question
A company is planning on using a cache service for their application. An important requirement is that trying to recover data lost in cache is an expensive affair for the company , which they would want to avoid. Which of the following should be used for this purpose?

 Amazon SQS queues
 Amazon ElastiCache – Memcached
 Amazon ElastiCache – Redis		-Correct
 Amazon Lambda
Unattempted
Answer – C
Choose Redis if you want high availability. The below table gives the comparison

Option A is incorrect since this is a queuing service available from AWS
Option B is incorrect since this does not offer high availability
Option D is incorrect since this is a serverless computing service available from AWS
For more information on the comparison for the Cache engines , please refer to the below URL
https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html



37. Question
You have docker containers which are going to be deployed in the AWS Elastic Container Service. You need to ensure that the underlying EC2 instances hosting the containers cannot access each other (since containers may be used by different customers). How can you accomplish this?

 Place IAM Roles for the underlying EC2 Instances
 Place the access keys in the Docker containers
 Place the access keys in the EC2 Instances
 Configure the Security Groups of the instances to allow only required traffic.		-Correct
Unattempted
Answer – D
Q: How does Amazon ECS isolate containers belonging to different customers?
Amazon ECS schedules containers for execution on customer-controlled Amazon EC2 instances or with AWS Fargate and builds on the same isolation controls and compliance that are available for EC2 customers. Your compute instances are located in a Virtual Private Cloud (VPC) with an IP range that you specify. You decide which instances are exposed to the Internet and which remain private.
Your EC2 instances use an IAM role to access the ECS service.
Your ECS tasks use an IAM role to access services and resources.
Security Groups and networks ACLs allow you to control inbound and outbound network access to and from your instances.
You can connect your existing IT infrastructure to resources in your VPC using industry-standard encrypted IPsec VPN connections.
You can provision your EC2 resources as Dedicated Instances. Dedicated Instances are Amazon EC2 Instances that run on hardware dedicated to a single customer for additional isolation.

Option A is incorrect since the Roles need to be assigned on the task level
Options B and C are incorrect since access keys is not the ideal security practise.
For more information on Task IAM Roles in ECS, please refer to the below URL:
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
For more information on ECS, please check below AWS Docs:
https://aws.amazon.com/ecs/faqs/



38. Question
An application needs to use an authentication in AWS. Users need to have MFA enabled when trying to log into the application. Which of the following can be used for this purpose?

 Create an IAM user with public access
 Create an IAM group with public access
 Use AWS Cognito with MFA		-Correct
 Use AWS STS with SAML
Unattempted
Answer – C
This is mentioned in the AWS Documentation
Adding Multi-Factor Authentication (MFA) to a User Pool
Multi-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on user name and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users.
With adaptive authentication, you can configure your user pool to require second factor authentication in response to an increased risk level. To add adaptive authentication to your user pool, see Adding Advanced Security to a User Pool.
Options A and B are incorrect since it’s not the right approach to use IAM users or groups for access for mobile based applications
Option D is incorrect since SAML is used for federated access.
For more information on Cognito with MFA , please refer to the below URL
https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html



39. Question
When calling an API operation on an EC2 Instance , the following error message was returned:

A client error (UnauthorizedOperation) occurred when calling the RunInstances operation:
You are not authorized to perform this operation. Encoded authorization failure message:
oGsbAaIV7wlfj8zUqebHUANHzFbmkzILlxyj__y9xwhIHk99U_cUq1FIeZnskWDjQ1wSHStVfdCEyZILGoccGpCiCIhORceWF9rRwFTnEcRJ3N9iTrPAE1WHveC5Z54ALPaWlEjHlLg8wCaB8d8lCKmxQuylCm0r1Bf2fHJRUjAYopMVmga8olFmKAl9yn_Z5rI120Q9p5ZIMX28zYM4dTu1cJQUQjosgrEejfiIMYDda8l7Ooko9H6VmGJXS62KfkRa5l7yE6hhh2bIwA6tpyCJy2LWFRTe4bafqAyoqkarhPA4mGiZyWn4gSqbO8oSIvWYPweaKGkampa0arcFR4gBD7Ph097WYBkzX9hVjGppLMy4jpXRvjeA5o7TembBR-Jvowq6mNim0

Which of the following can be used to get a human-readable error message?

 Use the command aws sts decode-authorization-message		-Correct
 Use the command aws get authorization-message
 Use the IAM Policy simulator , enter the error message to get the human readable format
 Use the command aws set authorization-message
Unattempted
Answer – A
This is mentioned in the AWS Documentation
Decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request.
For example, if a user is not authorized to perform an action that he or she has requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). Some AWS actions additionally return an encoded message that can provide details about this authorization failure
Because of the right command used in the documentation, all other options are incorrect
For more information on the command , please refer to the below URL
https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html



40. Question
Your company is hosting a set of resources on the AWS Cloud. There is now a security requirement that states that all requests to the STS service be monitored. How can you accomplish this requirement?

 Monitor the Cloudwatch logs service
 View logs in CloudTrail		-Correct
 Use the STS logging service
 Use Cloudwatch metrics
Unattempted
Answer – B
The AWS Documentation mentions the following
CloudTrail logs all authenticated API requests (made with credentials) to IAM and AWS STS APIs, with the exception of DecodeAuthorizationMessage. CloudTrail also logs nonauthenticated requests to the AWS STS actions, AssumeRoleWithSAML and AssumeRoleWithWebIdentity and logs information provided by the identity provider. You can use this information to map calls made by a federated user with an assumed role back to the originating external federated caller.
Option A is incorrect since the log service will not have the trail of the API calls
Option C is incorrect since STS does not have the logging service
Option D is incorrect since Cloudwatch metrics will not have the trail of the API calls
For more information on cloudtrail integrations , please refer to the below URL
https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html



41. Question
Your team has developed an application that makes use of AWS resources. In order to provide frequent releases to the customer, you are required to automate the CI/CD process. Which of the following can be used for this purpose?

 Create a pipeline using AWS Code deploy. Configure a stage for Unit testing as well in the pipeline
 Use AWS CodeCommit to host your code repository. Use the build tool in AWS CodeCommit to build your pipeline
 Create a Pipeline in the AWS CodeBuild Service
 Create a pipeline using AWS CodePipeline along with AWS Code Star service		-Correct
Unattempted
Answer – D

Automated continuous delivery pipeline
AWS CodeStar accelerates software release with the help of AWS CodePipeline, a continuous integration and continuous delivery (CI/CD) service. Each project comes pre-configured with an automated pipeline that continuously builds, tests, and deploys your code with each commit.

Option A is incorrect because pipelines are created using the “AWS Code pipeline” and “AWS Code deploy” automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers
Option B is incorrect since AWS CodeCommit does not have the facility in itself to carry out the build
Options C is incorrect since the CodePipeline service is used for building build pipelines.
For more information, please refer to the below URL
https://aws.amazon.com/codestar/features/



42. Question
Ensure that the IAM Role attached to the Instance has permission to upload data onto X-Ray

 Use the aws ssm get-parameters with the --with-decryption option		-Correct
 Use the aws ssm get-parameters with the --with-no-decryption option
 Give permissions to the AWS Code Deploy service via AWS Access Keys
 Give permissions to the AWS Code Deploy service via an IAM Role
Unattempted
43. Question
An application is currently accessing a DynamoDB table. Currently the tables queries are performing well. Changes have been made to the application and now the performance of the application is starting to degrade. After looking at the changes , you see that the queries are making use of an attribute which is not the partition key? Which of the following would be the adequate change to make to resolve the issue?

 Add a Global Secondary Index to the DynamoDB table		-Correct
 Change all the queries to ensure they use the partition key
 Enable global tables for DynamoDB
 Change the read capacity on the table
Unattempted



44. Question
Your team developed and deployed an application on an EC2 Instance. To test the application, you were given access credentials which also included the rights to write to an S3 bucket. Once the testing was confirmed, an IAM role was assigned to the instance. This role only has permissions to read, but you notice that the application still has access to write to the S3 bucket. Why is this the case?

 You need to restart the instance for the role settings to take effect
 The environment variables which were set for SDK access are taking priority		-Correct
 The CLI is corrupted, hence the credentials are not being revoked
 The EBS volume needs to be reattached again for the instance profile to take effect
Unattempted
Answer – B
Below is an excerpt from the documentation on how the credentials are evaluated when it comes to access. So when using the CLI , if the environment variables were set with the Access Keys , they would take preference over the IAM Role.
Using the Default Credential Provider Chain
When you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials by using the default credential provider chain implemented by the DefaultAWSCredentialsProviderChain class. The default credential provider chain looks for credentials in this order:

Environment variables–AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. The AWS SDK for Java uses the EnvironmentVariableCredentialsProvider class to load these credentials.
Java system properties–aws.accessKeyId and aws.secretKey. The AWS SDK for Java uses the SystemPropertiesCredentialsProvider to load these credentials.
The default credential profiles file– typically located at ~/.aws/credentials (location can vary per platform), and shared by many of the AWS SDKs and by the AWS CLI. The AWS SDK for Java uses the ProfileCredentialsProvider to load these credentials.
You can create a credentials file by using the aws configure command provided by the AWS CLI, or you can create it by editing the file with a text editor. For information about the credentials file format, see AWS Credentials File Format.
Amazon ECS container credentials– loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. The AWS SDK for Java uses the ContainerCredentialsProvider to load these credentials.

Instance profile credentials– used on EC2 instances, and delivered through the Amazon EC2 metadata service. The AWS SDK for Java uses the InstanceProfileCredentialsProvider to load these credentials.
Option A and D are incorrect since the IAM Role is instantly applied to the EC2 Instance
Option C is incorrect because even if the CLI is corrupted , still this would not be the cause of the underlying issue
For more information on an example on how credentials are evaluated , please refer to the below URL
https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html



45. Question
You are planning on deploying an application to the worker role in Elastic Beanstalk. Moreover, this worker application is going to run the periodic tasks. Which of the following is a must have as part of the deployment?

 An appspec.yaml file
 A cron.yaml file		-Correct
 A cron.config file
 An appspec.json file
Unattempted
Answer – B
This is also given in the AWS Documentation
Create an Application Source Bundle
When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you’ll need to upload a source bundle. Your source bundle must meet the following requirements:

Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)
Not exceed 512 MB
Not include a parent folder or top-level directory (subdirectories are fine)

If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file. For more information, see Periodic Tasks.

Because of the exact requirement given in the AWS Documentation, all other options are invalid.

For more information on creating an application source bundle for Elastic beanstalk , please refer to the below URL
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html



46. Question
You have a legacy application that works via XML messages. You need to place the application behind the API gateway in order for customers to make API calls. Which of the following would you need to configure?

 Enable Payload compression
 You will need to work with the Request and Response Data mapping		-Correct
 Enable CORS
 Enable multiple stages
Unattempted
Answer – B



47. Question
An application needs to make use of an SQS queue for working with messages. An SQS queue has been created with the default settings. The application needs 60 seconds to process each message. Which of the following step need to be carried out by the application.

 Change the VisibilityTimeout for each message and then delete the message after processing is completed.		-Correct
 Delete the message and change the visibility timeout.
 Process the message , change the visibility timeout. Delete the message
 Process the message and delete the message
Unattempted
Answer – A
If the SQS queue is created with the default settings , then the default visibility timeout is 30 seconds. And since the application needs more time for processing , you first need to change the timeout and delete the message after it is processed.
Option B is incorrect since you need to process the message first
Options C and D are incorrect since you need to change the visibility timeout for each message first
For more information on SQS visibility timeout , please refer to the below URL
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html



48. Question
An application currently makes use of DynamoDB tables. There are a thousand requests made per second on the DynamoDB table. Another application takes the changes to the items in the DynamoDB table, for further analytics processing. Which of the following can be effectively used to manage this requirement?

 Enable a scan on the entire table to check for changes
 Create a query to check for changes
 Enable global tables for DynamoDB
 Enable streams for DynamoDB		-Correct
Unattempted
Answer – D
The below information from the AWS Documentation helps to supplement this requirement
Capturing Table Activity with DynamoDB Streams
Many applications can benefit from the ability to capture changes to items stored in a DynamoDB table, at the point in time when such changes occur. Here are some example use cases:

An application in one AWS region modifies the data in a DynamoDB table. A second application in another AWS region reads these data modifications and writes the data to another table, creating a replica that stays in sync with the original table.
A popular mobile app modifies data in a DynamoDB table, at the rate of thousands of updates per second. Another application captures and stores data about these updates, providing near real time usage metrics for the mobile app.
A global multi-player game has a multi-master topology, storing data in multiple AWS regions. Each master stays in sync by consuming and replaying the changes that occur in the remote regions.
An application automatically sends notifications to the mobile devices of all friends in a group as soon as one friend uploads a new picture.
A new customer adds data to a DynamoDB table. This event invokes another application that sends a welcome email to the new customer.

DynamoDB Streams enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.
Options A and B are incorrect since these would result in a lot of throughput requirement for the table
Option C is incorrect since this is used for replication of data
For more information on DynamoDB streams , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html



49. Question
AWS CodeDeploy deployment fails to start & generate following error code, ”HEALTH_CONSTRAINTS_INVALID”, Which of the following can be used to eliminate this error?

 Make sure the minimum number of healthy instances is equal to the total number of instances in the deployment group.
 Increase the number of healthy instances required during deployment.
 Reduce number of healthy instances required during deployment.		-Correct
 Make sure the minimum number of healthy instances is greater than than the number of instances in the deployment group.
Unattempted
Correct Answer – C
AWS CodeDeploy generates ”HEALTH_CONSTRAINTS_INVALID” error, when a minimum number of healthy instances defined in deployment group are not available during deployment. To mitigate this error, make sure required number of healthy instances are available during deployments.
Option A is incorrect as During Deployment process, CodeDeploy tracks the health status of the instances in a deployment group. It uses the deployment’s specified minimum number of healthy instances to determine whether to continue the deployment. For this, minimum number of healthy instances should be less than & not equal to the total number of instances in the deployment group.
Option B is incorrect as to continue with deployment, you should increase the total number of instances in a deployment group as compared to minimum number of healthy instances.
Option D is incorrect as Number of healthy instances should be greater than & not equal or less than number of healthy instances specified in minimum number of healthy instances.

For more information on AWS CodeDeploy Error Codes, refer to the following URL,https://docs.aws.amazon.com/codedeploy/latest/userguide/error-codes.html



50. Question
Your application must write to an SQS queue. Your corporate security policies require that AWS credentials are always encrypted and are rotated at least once a week. How can you securely provide credentials that allow your application to write to the queue?

 Have the application fetch an access key from an Amazon S3 bucket at run time.
 Launch the application's Amazon EC2 instance with an IAM role.		-Correct
 Embed the Access keys in the application
 Create environment variables in the EC2 Instance with the Access Keys
Unattempted
Answer – B
This is clearly mentioned in the AWS Documentation
IAM Roles for Amazon EC2
Applications must sign their API requests with AWS credentials. Therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances. For example, you can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests, while protecting your credentials from other users. However, it’s challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS credentials.
We designed IAM roles so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles as follows:

Create an IAM role.
Define which accounts or AWS services can assume the role.
Define which API actions and resources the application can use after assuming the role.
Specify the role when you launch your instance, or attach the role to a running or stopped instance.
Have the application retrieve a set of temporary credentials and use them.

All other options are invalid because you should not use Access Keys , this is the recommended best practise.
For more information on IAM Roles , please refer to the below URL
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html



51. Question
You need to setup a RESTful API service in AWS that would be serviced via the following url:
https://example.com/customers
Which of the following combination of services can be used for development and hosting of the RESTful service? Choose 2 answers from the options below

 AWS Lambda and AWS API gateway		-Correct
 AWS S3 and Cloudfront
 AWS EC2 and AWS Elastic Load Balancer		-Correct
 AWS SQS and Cloudfront
Unattempted
Answer – A and C
AWS Lambda can be used to host the code and the API gateway can be used to access the API’s which point to AWS Lambda
Alternatively you can create your own API service , host it on an EC2 Instance and then use the AWS Application Load balancer to do path based routing.
Option B is incorrect since AWS S3 is normally is used to host static content
Option D is incorrect since AWS SQS is a queuing service
For more information on an example with RESTful API’s , please refer to the below URL
https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/



52. Question
Your company is planning on using the Simple Storage service to host objects that will be accessed by users. There is a speculation that there would be roughly 6000 GET requests per second. Which of the following could be used to ensure optimal performance? Choose 2 answers from the options given below?

 Use a Cloudfront distribution in front of the S3 bucket		-Correct
 Use sequential date-based naming for your prefixes.		-Correct
 Enable versioning for the objects
 Enable Cross Region Replication for the bucket
Unattempted
Answer – A and B
The AWS Documentation mentions the following on optimal performance for S3
Please refer page 2 on the below linkhttps://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf?trk=wp_card
Also you can use Cloudfront to give the objects to the user and cache them at the Edge locations , so that the requests on the bucket are reduced.

Option C is only used to prevent accidental deletion of objects
Option D is only used for disaster recovery scenarios
For more information on performance improvement, please refer to the below URLhttps://aws.amazon.com/blogs/aws/amazon-s3-performance-tips-tricks-seattle-hiring-event/

Note:
Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. It is simple to increase your read or write performance exponentially. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.
For more details, please check below AWS Docs: https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html



53. Question
Your company has asked you to maintain an application using Elastic Beanstalk. They have mentioned that when updates are made to the application, that the infrastructure maintains its full capacity. Which of the following deployment methods should you use for this requirement? Please select 2 correct options.

 All at once
 Rolling
 Immutable		-Correct
 Rolling with additional batch		-Correct
Unattempted
Answer – C and D
Since the only requirement is that the infrastructure should maintain its full capacity, So answers should be both C & D.

You can now use an immutable deployment policy when updating your application or environment configuration on Elastic Beanstalk. This policy is well suited for updates in production environments where you want to minimize downtime and reduce the risk from failed deployments. It ensures that the impact of a failed deployment is limited to a single instance and allows your application to serve traffic at full capacity throughout the update.

You can now also use a rolling with additional batch policy when updating your application. This policy ensures that the impact of a failed deployment is limited to a single batch of instances and allows your application to serve traffic at full capacity throughout the update.
Please refer to the following links for more information.https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-methodhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-imm



54. Question
Your team is developing a solution that will make use of DynamoDB tables. Currently, the application is designed to perform scans on the entire table. Which of the following can be done to improve the performance of the application when it interacts with the DynamoDB table? Choose 2 answers from the options given below

 Consider using parallel scans		-Correct
 Consider using large tables
 Consider using string partition keys
 Consider using queries		-Correct
Unattempted
Answer – A and D
The AWS Documentation mentions the following
Many applications can benefit from using parallel Scan operations rather than sequential scans. For example, an application that processes a large table of historical data can perform a parallel scan much faster than a sequential one. Multiple worker threads in a background “sweeper” process could scan a table at a low priority without affecting production traffic. In each of these examples, a parallel Scan is used in such a way that it does not starve other applications of provisioned throughput resources.
If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan
Option B is incorrect since having larger tables would just make the issue worse
Option C is incorrect since this would not help in the issue.
For more information on scans and queries, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html



55. Question
You have the following YAML file given to you which is required to deploy a Lambda function using serverless deployment.

AWSTemplateFormatVersion: ‘2010-09-09’
Transform: AWS::Serverless-2016-10-31
Resources:
TestFunction:
Type: AWS::Serverless::Function
Properties:
Handler: index.handler
Runtime: nodejs6.10
Environment:
Variables:
S3_BUCKET: demobucket

Which of the following is required to ensure the deployment can take place? Please select 2 correct answers.

 Use the cloudformation package command to package the deployment		-Correct
 Use the cloudformation package command to deploy the template
 Place the function code at the root level of the working directory along with the YAML file		-Correct
 Place the function code in the .eb extensions folder
Unattempted
Answer – A and C
The above snippet is used to create a serverless application that is deployed using the serverless deployment language. You need to ensure that the Lambda function is present as part of the deployment package
Option B is incorrect since these are not CloudFormation specific templates
Option D is incorrect since this is normally used for Elastic Beanstalk deployments

For more information on serverless deployment, please refer to the below URLhttps://docs.aws.amazon.com/lambda/latest/dg/serverless-deploy-wt.html



56. Question
You’ve define a DynamoDB table with a read capacity of 5 and a write capacity of 5. Which of the following statements are TRUE? Choose 3 answers from the options given below

 Strong consistent reads of a maximum of 20 KB per second		-Correct
 Eventual consistent reads of a maximum of 20 KB per second
 Strong consistent reads of a maximum of 40 KB per second
 Eventual consistent reads of a maximum of 40 KB per second		-Correct
 Maximum writes of 5KB per second		-Correct
Unattempted
Answer – A,D and E
This is also given in the AWS Documentation
For example, suppose that you create a table with 5 read capacity units and 5 write capacity units. With these settings, your application could:

Perform strongly consistent reads of up to 20 KB per second (4 KB × 5 read capacity units).
Perform eventually consistent reads of up to 40 KB per second (twice as much read throughput).
Write up to 5 KB per second (1 KB × 5 write capacity units).

Based on the documentation , all other options are incorrect
For more information on provisioned throughput , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html



57. Question
You have been instructed to use the CodePipeline service for the CI/CD automation in your company. Due to security reasons , the resources that would be part of the deployment are placed in another account. Which of the following steps need to be carried out to accomplish this deployment? Choose 2 answers from the options given below

 Define a customer master key in KMS		-Correct
 Create a reference Code Pipeline instance in the other account
 Add a cross account role		-Correct
 Embed the access keys in the codepipeline process
Unattempted
Answer – A and C
Option B is invalid since this would go against the security policy
Option D is invalid since this is not a recommended security practice.
This is mentioned in the AWS Documentation
You might want to create a pipeline that uses resources created or managed by another AWS account. For example, you might want to use one account for your pipeline and another for your AWS CodeDeploy resources. To do so, you must create a AWS Key Management Service (AWS KMS) key to use, add the key to the pipeline, and set up account policies and roles to enable cross-account access.
For more information on pipelines used to access resources in another account , please refer to the below URL
https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html



58. Question
You have deployed an application on an EC2 Instance. This application makes calls to a DynamoDB service. There are numerous performance issues present in the application. You decide to use the XRay service to debug the performance issues. You are not able to see the trails in the XRay service. Which of the following could be the underlying issue? Choose 2 answers from the options given below

 The X-Ray daemon is not installed on the EC2 Instance		-Correct
 The right AMI is not chosen for the EC2 Instance
 Ensure that the IAM Role attached to the Instance has permission to upload data ontoX-Ray		-Correct
 Ensure that the IAM Role attached to the Instance has permission to upload data onto Cloudwatch
Unattempted
Answer – A and C
You need to have the daemon service running on the EC2 Instance. And a role needs to be attached to the EC2 Instance
Running the X-Ray Daemon on Amazon EC2
You can run the X-Ray daemon on the following operating systems on Amazon EC2:
Amazon Linux
Ubuntu
Windows Server (2012 R2 and newer)

Use an instance profile to grant the daemon permission to upload trace data to X-Ray. For more information, see Giving the Daemon Permission to Send Data to X-Ray.
Option B is incorrect since the agent can be installed on different types of instances
Option D is incorrect since the traces need to be sent to the X-Ray service
For more information on the X-Ray daemon service , please refer to the below URL
https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html



59. Question
You are using the AWS CodeDeploy service to deploy an application onto AWS. The application uses secure parameters which are stored in the AWS Systems Manager Parameter store. Which of the following must be done, so that the deployment can be automated via CodeDeploy? Choose 2 answers from the options given below.

 Use the aws ssm get-parameters with the --with-decryption option		-Correct
 Use the aws ssm get-parameters with the --with-no-decryption option
 Give permissions to the AWS Code Deploy service via AWS Access Keys
 Give permissions to the AWS Code Deploy service via an IAM Role		-Correct
Unattempted
Answer – A and D
You need to specify the –with-decryption option, this allows the CodeDeploy service to decrypt the password so that it can be used in the application. Also, use IAM Roles to ensure the CodeDeploy service can access the KMS service
Option B is incorrect since you need to specify the –with-decryption option
Option C is incorrect since this is not a secure way to access AWS services

For more information on an example on this, please refer to the below URL:
https://aws.amazon.com/blogs/mt/use-parameter-store-to-securely-access-secrets-and-config-data-in-aws-codedeploy/



60. Question
You are developing a Java based application that needs to make use of the AWS KMS service for encryption. Which of the following must be done for the encryption and decryption process? Choose 2 answers from the options given below.

 Use the Customer master key to encrypt the data
 Use the Customer master key to generate a data key for the encryption process		-Correct
 Use the Customer master key to decrypt the data
 Use the generated data key to decrypt the data		-Correct
Unattempted
Answer – B and D
The AWS Documentation mentions the following
The AWS Encryption SDK is a client-side encryption library that makes it easier for you to implement cryptography best practices in your application. It includes secure default behaviour for developers who are not encryption experts, while being flexible enough to work for the most experienced users.
Options A and C are incorrect because you should never use the Customer master keys directly for the encryption of decryption process.
In the AWS Encryption SDK, by default, you generate a new data key for each encryption operation
For more information on the Encryption SDK , please refer to the below URL
https://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html

Note:
AWS Docs Says
“When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.
You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the master key.”
For more information on the enveloping, please refer to the below URL
https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping



61. Question
You have developed a Web based application which makes calls to backend API. Which of the following headers are required by Web browsers to be set up in each API method which has CORS enabled.(Select TWO.)

 Access-Control-Allow-Headers		-Correct
 Access-Control-Allow-CORS
 Access-Control-Expose-Headers
 Access-Control-Expose-Origin
 Access-Control-Allow-Origin		-Correct
Unattempted
Correct Answer – A, E
To support CORS, API resource needs to implement an OPTIONS method that can respond to the OPTIONS preflight request with following headers,
Access-Control-Allow-Headers
Access-Control-Allow-Origin
Access-Control-Allow-Methods
Option B, C & D are incorrect as both these headers are not required to included as a part of OPTIONS method.
For more information on enabling CORS on resource using API Gateway, refer to the following URL,
https://docs.aws.amazon.com/apigateway/latest/developerguide/enable-cors-for-resource-using-swagger-importer-tool.html



62. Question
You are planning on using the Serverless Application model which will be used to deploy a serverless application consisting of a Node.js function. Which of the following steps need to be carried out? Choose 2 answers from the options given below.

 Use the Lambda package command
 Use the SAM package command		-Correct
 Use the Lambda deploy command
 Use the SAM deploy command		-Correct
Unattempted
Answer – B and D



63. Question
Your application currently points to several Lambda functions in AWS. A change is being made to one of the Lambda functions. You need to ensure that application traffic is shifted slowly from one Lambda function to the other. Which two of the following steps would you carry out?

 Create an ALIAS with the –routing-config parameter		-Correct
 Update the ALIAS with the –routing-config parameter		-Correct
 Create a version with the –routing-config parameter
 Update the version with the –routing-config parameter
 Update the function with the - config parameter
Unattempted
Answer – A and B
This is mentioned in the AWS Documentation
By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version.
Options C and D are incorrect since you need to use ALIAS for this purpose.
Option E is incorrect. Because A & B are the correct ways to achieve the requirement.
For more information on shifting traffic using ALIAS , please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html



64. Question
You are developing an application that would be used to upload images from users. You need to effectively store the images and also the name of the user who uploaded the image. How would you accomplish this? Choose 2 answers from the options given below.

 Store the images in DynamoDB
 Store the images in S3		-Correct
 Store the name of the user in S3
 Store the name of the user in DynamoDB		-Correct
Unattempted
Answer – B and D
This is also mentioned in the AWS Documentation
As mentioned above, you can also take advantage of Amazon Simple Storage Service (Amazon S3) to store large attribute values that cannot fit in a DynamoDB item. You can store them as an object in Amazon S3 and then store the object identifier in your DynamoDB item.
Options A and C are incorrect since it should be the other way around in terms of the storage
For more information on this use case , please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html



65. Question
Your company has asked you to maintain an application using Elastic Beanstalk. At times , you normally hit the application version limit when deploying new versions of the application. Which of the following is the most effective way to manage this issue?

 Create multiple environments and deploy the different versions to different environments
 Create an application lifecycle policy		-Correct
 Create multiple applications and deploy the different versions to different applications
 Delete the application versions manually
Unattempted
Answer – B
The AWS Documentation mentions the following
Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don’t delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.
You can avoid hitting the limit by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete application versions that are old, or to delete application versions when the total number of versions for an application exceeds a specified number.
Options A and C are invalid because these are not the right approaches when managing deployment of application versions.
Option D even though possible , is not the most effective way.
For more information on application lifecycle , please refer to the below URL
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html