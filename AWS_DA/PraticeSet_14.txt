1. Question
You are a developer for a company that has been given the responsibility to debug performance issues for an existing application. The application connects to a MySQL RDS Instance in AWS. There is a suspicion that there are performance issues in the underlying queries. Which of the following can help diagnose these issues?

 The Cloudtrail logs for the region
 Get the slow query logs for the RDS service		-Correct
 Use the AWS Config service to diagnose the problem areas
 Use the AWS Inspector service to diagnose the problem areas
Unattempted
Answer – B
The AWS RDS Service contains several logs such as the ones given below
Error log – contains diagnostic messages generated by the database engine, along with startup and shutdown times.
General query log – contains a record of all SQL statements received from clients, and also client connect and disconnect times.
Slow query log – contains a record of SQL statements that took longer to execute than a set amount of time and that examined more than a defined number of rows. Both thresholds are configurable.

Option A is incorrect because this is used to monitor API activity
Option C is incorrect because this is used as a configuration service
Option D is incorrect because this is used to inspect EC2 Instances for vulnerabilities
For more information on monitoring Amazon RDS , please refer to the below URL
https://aws.amazon.com/blogs/database/monitor-amazon-rds-for-mysql-and-mariadb-logs-with-amazon-cloudwatch/



2. Question
Your team has an application deployed using the Elastic Beanstalk service. A Web environment has been configured for the production environment. There is now a requirement to perform a Blue Green deployment for a new version of the application. How can you achieve this?

 Create a new application and swap the application environments.
 Create a new application version and upload the new application version
 Create a new environment in the application with the updated application version and perform a swap		-Correct
 Create a new environment in the application and Load the configuration of an existing environment
Unattempted
Answer – C
This is mentioned in the AWS Documentation

Since this is clearly mentioned in the AWS Documentation, all other options are invalid

For more information on Blue Green deployments in Elastic Beanstalk, please refer to the below URL
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html



3. Question
You are developing an application that consist of the following architecture
· A set of EC2 Instances hosting a web layer
· A database hosting a MySQL instance
You need to add a layer that can be used to ensure that the most frequently accessed data from the database is fetched in a more fast and efficient manner from the database. Which of the following can be used to accomplish this requirement?

 An SQS queue to store the frequently accessed data
 An SNS topic to store the frequently accessed data
 A Cloudfront distribution to store the frequently accessed data
 A Elasticache instance to store the frequently accessed data		-Correct
Unattempted
Answer – D
The AWS Documentation mentions the following
Amazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, run, and scale popular open source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for Gaming, Ad-Tech, Financial Services, Healthcare, and IoT apps.
Option A is incorrect because this is a messaging service
Option B is incorrect because this is a notification service
Option C is incorrect because this is a web distribution service
For more information on Amazon ElastiCache, please refer to the below URL
https://aws.amazon.com/elasticache/



4. Question
You’ve just started development on an application that will make use of the ElastiCache service. You need to ensure that objects are cached but not kept inadvertently for a long time. Which of the following cache maintenance strategy would you employ for the cache service?

 TTL		-Correct
 Lazy Loading
 Write Through
 Read Through
Unattempted
Answer – A
The AWS Documentation mentions the following
Lazy loading allows for stale data but won’t fail with empty nodes. Write through ensures that data is always fresh but may fail with empty nodes and may populate the cache with superfluous data. By adding a time to live (TTL) value to each write, we are able to enjoy the advantages of each strategy and largely avoid cluttering up the cache with superfluous data.
Option B is incorrect because this is a caching strategy that loads data into the cache only when necessary.
Option C is incorrect because this is a caching strategy that adds data or updates data in the cache whenever data is written to the database.
Option D is incorrect because there is no such caching strategy
For more information on Caching strategies, please refer to the below URL
https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html



5. Question
Your team has been tasked with the development of an application. The backend store needs to store data on which ad-hoc queries can run and table joins are required. Which of the following would you use to host the underlying data store?

 AWS RDS		-Correct
 AWS DynamoDB
 AWS S3
 AWS Athena
Unattempted
Answer – A
The AWS Documentation gives an example comparison of when to use AWS RDS for SQL data and DynamoDB for NoSQL data

Option B is incorrect because this should not be used when you have table joins to be carried out.

Option C is incorrect because this is used for object level storage
Option D is incorrect because this is used for querying data in S3
For more information on when to use SQL over NoSQL, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html



6. Question
Your team has an application being served out of AWS S3. There is a surge of increased number of GET requests. After monitoring using Cloudwatch metrics you can see the rate of GET requests going close to 5000 requests per second. Which of the following can be used to ensure better performance?

 Add an Elasticache in front of the S3 bucket
 Use DynamoDB instead of using S3
 Place a Cloudfront distribution in front of the S3 bucket		-Correct
 Place an Elastic Load balancer in front of the S3 bucket
Unattempted
Answer – C
The AWS Documentation mentions the following
If your workload is mainly sending GET requests, in addition to the preceding guidelines, you should consider using Amazon CloudFront for performance optimization. By integrating CloudFront with Amazon S3, you can distribute content to your users with low latency and a high data transfer rate. You also send fewer direct requests to Amazon S3, which reduces your costs.
Since the documentation clearly mentions this, all other options are invalid
For more information, please check the below link.https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf (page 523)



7. Question
Your team is planning on creating a Lambda function which will interact with a DynamoDB stream. Which of the following would need to be in place to ensure the Lambda function can interact with the DynamoDB table.

 Access Keys for an IAM user embedded in the function
 IAM Role with required permissions to access DynamoDB		-Correct
 The password for an IAM user in the environment variable for the Lambda function
 A Security group rule to allow egress traffic into DynamoDB
Unattempted
Answer – B
The AWS Documentation mentions the following
Regardless of what invokes a Lambda function, AWS Lambda always executes a Lambda function on your behalf. If your Lambda function needs to access any AWS resources, you need to grant the relevant permissions to access those resources. You also need to grant AWS Lambda permissions to poll your DynamoDB stream. You grant all of these permissions to an IAM role (execution role) that AWS Lambda can assume to poll the stream and execute the Lambda function on your behalf. You create this role first and then enable it at the time you create the Lambda function
For more information on using Lambda with DynamoDB, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html



8. Question
You have created a Lambda State Function which is generating error “ServiceException”, Which of the following is a best practice to handle this exception with a Lambda State Function?

 Use Lambda Catch code with only “ErrorEquals” string.
 Use Lambda Retry code with only “BackoffRate” string
 Use Lambda Retry code with only “ErrorEquals” string.		-Correct
 Use Lambda Catch code with only “ResultPath” string.
Unattempted
Correct Answer – C
For errors such as “ServiceException”, best practice is to Retry invoking Lambda function. Within a Retry Code “ErrorEquals” field is required string which matches error names & all other fields are optional.
Option A is incorrect as Lambda Catch code is only used after a number of retries are performed by State function.
Option B is incorrect as BackoffRate field is optional in Lambda Retry code & if not specified Default value of 2.0 is considered.
Option D is incorrect as Lambda Catch code is only used after a number of retries are performed by State function. ResultPath is an optional field in a Catch Code, ErrorEquals & Next are required strings.
For more information on troubleshooting Lambda State Function errors, refer to the following URL,
https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html#amazon-states-language-retrying-after-error



9. Question
Your team is using the AWS CodeBuild service for an application build. As part of Integration testing during the build phase, the application needs to access an RDS instance in a private subnet. How can you ensure this is possible?

 Create a VPC Endpoint and ensure the codebuild project configuration is modified with the endpoint information
 Provide additional VPC-specific configuration information as part of your AWS CodeBuild project		-Correct
 Mark the subnet as a public subnet during the time of the Integration tests
 Use a NAT gateway to relay the requests from AWS CodeBuild to the RDS Instance
Unattempted
Answer – B
This is given in the AWS Documentation
Typically, resources in an VPC are not accessible by AWS CodeBuild. To enable access, you must provide additional VPC-specific configuration information as part of your AWS CodeBuild project configuration. This includes the VPC ID, the VPC subnet IDs, and the VPC security group IDs. VPC-enabled builds are then able to access resources inside your VPC.
VPC connectivity from AWS CodeBuild builds makes it possible to:
Run integration tests from your build against data in an Amazon RDS database that’s isolated on a private subnet.
Query data in an Amazon ElastiCache cluster directly from tests.
Interact with internal web services hosted on Amazon EC2, Amazon ECS, or services that use internal Elastic Load Balancing.

Since the requirements are clearly mentioned in the documentation, all other options are incorrect
For more information on VPC support for AWS CodeBuild, please refer to the below URL
https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html



10. Question
You have created a web-application which saves all data in Amazon S3 buckets. If you have created 3 prefixes in a S3 bucket, what will be PUT & GET read performance you will get per second?

 3,500 PUT and 5,500 GET requests per second.
 7,000 PUT and 11,000 GET requests per second.
 10,500 GET and 16,500 PUT requests per second.
 10,500 PUT and 16,500 GET requests per second.		-Correct
Unattempted
Correct Answer – D
Amazon S3 provides 3,500 PUT requests per second per prefix in a bucket & 5,500 GET request per second per prefix in a bucket. So, for 3 prefixes, S3 bucket will support 10,500 PUT and 16,500 GET requests per second.
Option A is incorrect as 3,500 PUT and 5,500 GET requests per second are provided for each prefix.
Option B is incorrect as 7,000 PUT and 11,000 GET requests per second will be provided for 2 prefixes & not for 3 prefixes.
Option C is incorrect as for 3 prefixes, performance which you will get is 10,500 PUT and 16,500 GET requests per second & not 10,500 GET and 16,500 PUT requests per second
For more information on PUT & GET performance with Amazon S3 bucket, refer to the following URL,
https://docs.aws.amazon.com/AmazonS3/latest/dev/PerformanceOptimization.html



11. Question
Your team has developed an application that will be launched on EC2 Instances that are part of the an Autoscaling Group. It needs to be ensured that the application can get the IP address of the Instance. How can you achieve this?

 Make the application query the Instance Metadata		-Correct
 Make the application query the Instance Userdata
 Make the application query the Autoscaling Group
 Make the application query the Launch configuration
Unattempted
Answer – A
This is given in the AWS Documentation
Instance metadata is data about your instance that you can use to configure or manage the running instance
Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you’re writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.
Since the data can only be retrieved from the Instance Metadata , all other options are invalid
For more information on Instance Metadata, please refer to the below URL
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html



12. Question
You are a developer for a company. You are planning to use the X-Ray service to trace all incoming HTTP requests for an application being developed. In the X-Ray SDK which of the following feature would you use to fulfil this requirement?

 Interceptors		-Correct
 Client handlers
 Server handlers
 Daemon service
Unattempted
Answer – A
This is given in the AWS Documentation
The X-Ray SDK provides:
Interceptors to add to your code to trace incoming HTTP requests
Client handlers to instrument AWS SDK clients that your application uses to call other AWS services
An HTTP client to use to instrument calls to other internal and external HTTP web services

Since this is clearly given in the documentation, all other options are incorrect
For more information on using AWS X-Ray, please refer to the below URLhttps://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html



13. Question
You’re planning on using the DataPipeline service to transfer data from Amazon S3 to Redshift. You need to define the source and destination locations. Which of the following part of the DataPipeline service allows you to define these locations?

 Data Nodes		-Correct
 Task Runner
 Activities
 Resources
Unattempted
Answer – A
This is given in the AWS Documentation

Option B is incorrect since the Task Runner is an application that polls AWS Data Pipeline for tasks and then performs those tasks.

Option C is incorrect since an activity is a pipeline component that defines the work to perform
Option D is incorrect since a resource is the computational resource that performs the work that a pipeline activity specifies
For more information on Data Nodes, please refer to the below URL
https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html



14. Question
As a database developer, you have started working with Redshift. Your IT administrator has provisioned a Redshift cluster. You now need to load data into the Redshift cluster from S3. Which of the following command should you use for this activity?

 GET
 COPY		-Correct
 MERGE
 PUT
Unattempted
Answer – B
This is given in the AWS Documentation
The ideal command to be used is given in the documentation , hence all other options are incorrect
For more information on working with a sample Redshift cluster, please refer to the below URL
https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-sample-db.html
https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html
https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html



15. Question
You are currently a Business Intelligence developer for a company. A lot of data sources that are defined for the Logistics team in your company is hosted in AWS. They are now looking for a quick solution to build visualization screens around the data hosted in AWS. Which of the following can be used to fulfil this requirement?

 AWS Redshift
 AWS Quicksight		-Correct
 AWS Glue
 AWS DynamoDB
Unattempted
Answer – B
This is given in the AWS Documentation
Amazon QuickSight is a business analytics service you can use to build visualizations, perform ad hoc analysis, and get business insights from your data. It can automatically discover AWS data sources and also works with your data sources. Amazon QuickSight enables organizations to scale to hundreds of thousands of users and delivers responsive performance by using a robust in-memory engine (SPICE).
Option A is invalid because this is a datawarehousing solution
Option C is invalid because this service is fully managed ETL (extract, transform, and load) service
Option D is invalid because this is a fully managed NoSQL database
For more information on AWS Quicksight, please refer to the below URL
https://docs.aws.amazon.com/quicksight/latest/user/welcome.html



16. Question
You are setting out policies for allowing users access to objects in an S3 bucket. You have configured a policy for testing which currently works as intended. You try to create a more restrictive policy but find out that the changes are not working as intended. What can you do to resolve the issue in the EASIEST way possible?

 Delete the current version of the policy and recreate the older one
 Revert to the previous version of the policy		-Correct
 Recreate the IAM users again
 Use the recycle bin to get the deleted policies back
Unattempted
Answer – B
The AWS Documentation mentions the following
You create a customer managed policy that allows users to administer a particular Amazon S3 bucket using the AWS Management Console. Upon creation, your customer managed policy has only one version, identified as v1, so that version is automatically set as the default. The policy works as intended.
Later, you update the policy to add permission to administer a second Amazon S3 bucket. IAM creates a new version of the policy, identified as v2, that contains your changes. You set version v2 as the default, and a short time later your users report that they lack permission to use the Amazon S3 console. In this case, you can roll back to version v1 of the policy, which you know works as intended. To do this, you set version v1 as the default version. Your users are now able to use the Amazon S3 console to administer the original bucket.
Because the AWS Documentation clearly mentions this , all other options are invalid.
For more information on the Amazon Container Service, please refer to the below URL
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-versioning.html



17. Question
Your development team is planning on using the AWS Batch service to process a high number of intensive performance computing jobs. Which of the following integration services with AWS Batch will allow you to monitor the progress of the jobs?

 AWS Cloudtrail
 AWS Cloudwatch Events		-Correct
 AWS Config
 AWS SQS
Unattempted
Answer – B
The AWS Documentation mentions the following
Using CloudWatch Events, you can monitor the progress of jobs, build AWS Batch custom workflows with complex dependencies, generate usage reports or metrics around job execution, or build your own custom dashboards. With AWS Batch and CloudWatch Events, you can eliminate scheduling and monitoring code that continuously polls AWS Batch for job status changes. Instead, handle AWS Batch job state changes asynchronously using any CloudWatch Events target, such as AWS Lambda, Amazon Simple Queue Service, Amazon Simple Notification Service, or Amazon Kinesis Data Streams.
Option A is incorrect since this is an API Monitoring service
Option C is incorrect since this is used to monitor configuration changes
Option D is incorrect since this is used as a messaging service
For more information on Cloudwatch event stream, please refer to the below URL
https://docs.aws.amazon.com/batch/latest/userguide/cloudwatch_event_stream.htm



18. Question
Your team is working on an application that will connect to a MySQL RDS Instance. The security mandate is that the connection to the database from the application should be encrypted. How can you accomplish this?

 By using Access Keys assigned to an IAM user
 By using Private Key pairs
 By using SSL		-Correct
 By using KMS Keys
Unattempted
Answer – C
The AWS Documentation mentions the following

Option A is incorrect since this is used for programmatic access for a user
Option B is incorrect since this is used for connection to an EC2 Instance
Option D is incorrect since is normally used for encrypting data at rest or before data is sent in transit
For more information on using RDS with SSL, please refer to the below URLhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html



19. Question
You are planning on developing and deploying a Node.js Lambda function. The code has a dependency on a lot of third-party libraries. Which of the following needs to be done to ensure the code can be executed in the AWS Lambda service?

 Install the third-party libraries in the Lambda service
 Create a deployment package with your code and the third-party libraries		-Correct
 Use Cloudformation templates to deploy the third-party libraries
 Use an IAM Role with the required permissions on those libraries
Unattempted
Answer – B
The AWS Documentation mentions the following
If you are writing code that uses other resources, such as a graphics library for image processing, or you want to use the AWS CLI instead of the console, you need to first create the Lambda function deployment package, and then use the console or the CLI to upload the package.
Because of what is mentioned in the AWS Documentation, all other options are invalid
For more information on creating the deployment package, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html



20. Question
You have to create a DynamoDB table called Customers which will have 2 attributes. One is the ID which will be the partition key and the other is the Name which will be the sort key. Which of the following is the right definition for the CLI command that would be used to create the table?

 aws dynamodb create-table \ --table-name Customers \ --attribute-definitions \
 
 AttributeName=ID,AttributeType=N \ AttributeName=Name,AttributeType=S \ --key-schema \ AttributeName=ID,KeyType=HASH \ AttributeName=Name,KeyType=RANGE \ --provisioned-throughput \ ReadCapacityUnits=10,WriteCapacityUnits=5		-Correct
 
 aws dynamodb create-table \ --table-name Customers \ --attribute-definitions \ AttributeName=ID,AttributeType=N\ AttributeName=Name,AttributeType=S \ --provisioned-throughput \ ReadCapacityUnits=10,WriteCapacityUnits=5
 
 aws dynamodb create-table \ --table-name Customers \ --attribute-definitions \ AttributeName=ID,AttributeType=N \ AttributeName=Name,AttributeType=S \ --key-schema \ AttributeName=Name,KeyType=HASH \ AttributeName=ID,KeyType=RANGE \ --provisioned-throughput \ ReadCapacityUnits=10,WriteCapacityUnits=5
 
 aws dynamodb set-table \ --table-name Customers \ --attribute-definitions \ AttributeName=ID,AttributeType=N \ AttributeName=Name,AttributeType=S \ --key-schema \ AttributeName=ID,KeyType=HASH \ AttributeName=Name,KeyType=RANGE \ --provisioned-throughput \ ReadCapacityUnits=10,WriteCapacityUnits=5
 
 aws dynamodb set-table \ --table-name Customers \ --attribute-definitions \ AttributeName=ID,AttributeType=N \ AttributeName=Name,AttributeType=S \ --key-schema \ AttributeName=ID,KeyType=HASH \ AttributeName=Name,KeyType=RANGE \ --provisioned-throughput \ ReadCapacityUnits=10,WriteCapacityUnits=5
 
Unattempted
Answer – A
An example of this is given in the AWS Documentation

Option B is incorrect since the keys are not being defined here
Option C is incorrect since the Name key should be RANGE and the ID should be HASH
Option D is incorrect since the right CLI command is create-table
For more information on working with tables, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.Basics.html



21. Question
You are using a custom tool known as POSTMAN to make API requests to resources in AWS. Part of the job of sending requests is to sign the request. Which of the following would you use to sign the API requests made to AWS?

 Your user name and password
 A private key file
 KMS keys
 Access Keys		-Correct
Unattempted
Answer – D
The AWS Documentation mentions the following
When you send HTTP requests to AWS, you sign the requests so that AWS can identify who sent them. You sign requests with your AWS access key, which consists of an access key ID and secret access key. Some requests do not need to be signed, such as anonymous requests to Amazon Simple Storage Service (Amazon S3) and some API operations in AWS Security Token Service (AWS STS) such as AssumeRoleWithWebIdentity.
Option A is incorrect since this is used for console-based access
Option B is incorrect since this is used for logging onto EC2 Instances
Option C is incorrect since this is use for encrypting data
For more information on signing API requests, please refer to the below URL
https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html



22. Question
A company is making use of the Simple Notification service to send notifications to various subscribers for their service. There is a user requirement for the subscriber to only receive certain types of messages and not all messages published to the topic. How can you achieve this?

 By adding a filter policy to the topic subscription		-Correct
 By adding an IAM policy to the topic
 Publish the messages to an SQS queue
 Publish the messages to a Lambda function
Unattempted
Answer – A
The AWS Documentation mentions the following
By default, a subscriber of an Amazon SNS topic receives every message published to the topic. To receive only a subset of the messages, a subscriber assigns a filter policy to the topic subscription.
A filter policy is a simple JSON object. The policy contains attributes that define which messages the subscriber receives. When you publish a message to a topic, Amazon SNS compares the message attributes to the attributes in the filter policy for each of the topic’s subscriptions. If there is a match between the attributes, Amazon SNS sends the message to the subscriber. Otherwise, Amazon SNS skips the subscriber without sending the message to it. If a subscription lacks a filter policy, the subscription receives every message published to its topic.
Since the documentation clearly mentions this , all other options are incorrect
For more information on message filtering, please refer to the below URL
https://docs.aws.amazon.com/sns/latest/dg/message-filtering.html



23. Question
Your company has an existing Redshift cluster. The sales team currently store historical data in the cluster. There is now a requirement to ensure that all data is encrypted at rest. What do you need to do on your end (as of October 2018)?

 Enable the encryption feature for the cluster.		-Correct
 Enable encryption for the underlying EBS volumes.
 Use SSL certificates to encrypt the data at rest.
 Create a new cluster with encryption enabled and then migrate the data over to the new cluster.
Unattempted
Answer – A
The AWS Documentation mentions the following:
In Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest. When you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.
Encryption is an optional, immutable setting of a cluster. If you want encryption, you enable it during the cluster launch process. As of October 2018, you can enable encryption on an un-encrypted cluster and AWS will handle migrating the data over to a new, encrypted cluster behind-the-scenes.
Option A is CORRECT because you can now enable encryption for an existing Redshift cluster, Please refer the below link
https://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html
Option B is invalid since the encryption needs to be enabled at the cluster level
Option C is invalid since SSL certificates are used for encryption of data in transit
Option D is incorrect because you can now enable encryption for an existing Redshift cluster and therefore creating a new Redshift cluster to enable encryption is not necessary



24. Question
Your team has developed a web application that will run on an EC2 Instance. There is a deployment requirement wherein if the primary application fails, the requests need to be routed to a static web site. Which of the following can help you achieve this?

 A Classic Load balancer placed in front of the EC2 Instances
 An Application Load balancer placed in front of the EC2 Instances
 A health check in Route 53		-Correct
 The swap URL feature in Elastic Beanstalk
Unattempted
Answer – C
The AWS Documentation mentions the following
Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following:
The health of a specified resource, such as a web server
The status of other health checks
The status of an Amazon CloudWatch alarm
Options A and B are incorrect since the Load balancers are used to distribute traffic and not divert traffic
Option D is incorrect since the application is not being hosted on Elastic Beanstalk
For more information on DNS failover, please refer to the below URL
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html



25. Question
Your team has decided to host a static web site using the Simple Storage Service. A bucket has been defined with the domain name, the objects uploaded, and the static web site hosting enabled for the bucket. But you are still not able to access the web site. Which of the following could be the underlying issue?

 You need to enable versioning for the bucket as well
 The bucket must have public read access		-Correct
 You need to ensure the storage class is infrequent access
 You need to use AWS Managed Keys
Unattempted
Answer – B
The AWS Documentation mentions the following
To host a static website, you configure an Amazon S3 bucket for website hosting, and then upload your website content to the bucket. This bucket must have public read access. It is intentional that everyone in the world will have read access to this bucket.
Option A is incorrect since this feature is used to avoid accidental deletion of objects
Option C is incorrect since this the storage class should ideally be standard storage
Option D is incorrect since this is used for encryption of objects at rest
For more information on static web site hosting, please refer to the below URL
https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html



26. Question
Your application is currently configured to interact with an S3 bucket. Now you are getting errors that the bucket does not exist. Which of the following is the best way to understand how the bucket was deleted?

 Use the Cloudwatch logs to see the Bucket Deletion API request
 Use the Cloudtrail logs to see the Bucket Deletion API request		-Correct
 Use the AWS Inspector service to see the Bucket Deletion API request
 Use the AWS Trusted Advisor service to see the Bucket Deletion API request
Unattempted
Answer – B
You can use the Cloudtrail service to see when the bucket was deleted and who initiated the bucket deletion request.

Option A is incorrect since the logs will not have the detailed information about the bucket deletion request
Option C is incorrect since this service is only used to check the vulnerabilities on servers
Option D is incorrect since this service is only used to provide recommendations
For more information on Cloudtrail logging, please refer to the below URL
https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html



27. Question
Your team is developing an application which makes use of Docker containers. These containers will be deployed to the Elastic Container Service. The applications on these containers need to interact with DynamoDB tables. Which of the following is the most secure way to ensure the containers can interact with DynamoDB?

 Create an IAM Role for the ECS Tasks		-Correct
 Embed the Access Keys in the containers
 Embed the Access Keys in the cluster
 Use an IAM user’s credentials to spin up the cluster
Unattempted
Answer – A
The AWS Documentation mentions the following
With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.
All other options are invalid since the most secure way is to use IAM Roles for accessing AWS services
For more information on IAM Roles for tasks, please refer to the below URL
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html



28. Question
An application is making calls to a DynamoDB table. The queries are taking on a lot of read capacity. The table has a large number of attributes. Not all of the attributes are used in the query. Which of the following can be used to minimize the read capacity being used by the queries?

 Use global secondary indexes with projected attributes		-Correct
 Use an Application Load balancer in front of the DynamoDB table
 Consider using parallel scans on the table
 Use a cloudfront distribution in front of the DynamoDB table
Unattempted
Answer – A
You can use Global secondary indexes and use only those attributes which will be queried. This can help reduce the amount of read throughput used on the table.
Options B and D are invalid because these are invalid architecture designs
Option C is incorrect since using queries is more effective
For more information on general guidelines for indexes, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html



29. Question
Your team is developing a set of Lambda functions. You need to ensure the team uses the best practices for working with AWS Lambda. What is the advantage of initializing any external dependencies of your Lambda function code? Choose one of the options given below.

 Ability to decode errors better
 Ability to instantiate an object for each invocation
 Ability to reuse Execution Context		-Correct
 Ability to call functions faster
Unattempted
Answer – C
This is given as one of the best practises for AWS Lambda in the documentation
Take advantage of Execution Context reuse to improve the performance of your function. Make sure any externalized configuration or dependencies that your code retrieves are stored and referenced locally after initial execution. Limit the re-initialization of variables/objects on every invocation. Instead use static initialization/constructor, global/static variables and singletons. Keep alive and reuse connections (HTTP, database, etc.) that were established during a previous invocation.
Since this is clearly given in the AWS Documentation, all other options are invalid
For more information on Best practices, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html
https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html



30. Question
An application includes an Autoscaling Group. It has been determined that the best way to scale the group is based on the number of concurrent users. How can you achieve this?

 Create a tag for the Group to contain the number of concurrent users
 Create a custom metric for the number of concurrent users		-Correct
 Since concurrent user metrics are not available, base the scaling of the group on CPU percentage
 Since concurrent user metrics are not available, base the scaling of the group on Memory percentage
Unattempted
Answer – B
The AWS Documentation mentions the following
With target tracking scaling policies, you select a predefined metric or configure a customized metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern and minimizes changes to the capacity of the scalable target.
Since you can define a custom metric based on concurrent usage, you can scale the Autoscaling group based on that. Based on this all other options are invalid.
For more information on auto scaling based on target tracking, please refer to the below URL
https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html



31. Question
You have an application storing objects in Amazon S3 bucket. Due to massive popularity, these objects are frequently accessed from S3 bucket. Which of the following can be used to enhance the GET performance and latency of these objects?

 Save objects in separate buckets in each region.
 Use Amazon CloudFront in front of S3 bucket.		-Correct
 Increase number of prefixes in S3 bucket.
 Add random suffix to key names
Unattempted
Correct Answer – B
For objects stored in Amazon S3 bucket, to achieve a very high GET performance Amazon CloudFront can be used. Using Amazon CloudFront, users can access data locally with low latency & high throughput from nearest POP. Also, this results in a decrease in the number of read requests directly to Amazon S3 buckets.
Option A is incorrect as although this will lower latency for users accessing from buckets within the same region, but this is not a cost-effective solution.
Option C is incorrect as although this will increase GET performance, this will not result in low latency for users.
Option D is incorrect as we need to add prefix and not a suffix to the key names
For more information on Performance Optimisation with Amazon S3, refer to the following URL,
https://docs.aws.amazon.com/AmazonS3/latest/dev/PerformanceOptimization.html



32. Question
Your company has a SOAP service that receives requests in XML. The service is going to be placed behind the API gateway service. Which of the following must be done to ensure that requests made to the API gateway service can be consumed by the SOAP service?

 Create a mapping template		-Correct
 Setup a gateway response
 Enable support for binary workloads
 Setup Private integrations
Unattempted
Answer – A
The AWS Documentation mentions the following
In API Gateway, an API’s method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.
A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. The payload can have a data model according to the JSON schema draft 4. You must define the model in order to have API Gateway to generate a SDK or to enable basic request validation for your API. You do not have to define any model to create a mapping template. However, a model can help you create a template because API Gateway will generate a template blueprint based on a provided model.
Option B is incorrect since this is used to customize gateway responses
Option C is incorrect since this is used specifically for binary workloads
Option D is incorrect since this is used for use within a VPC
For more information on models and mappings, please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html



33. Question
An application needs to make use of the SQS service for sending and receiving messages. The application takes 60 seconds to process a message. Assuming that a queue has been created with the default settings, which one of the following must be implemented?

 Call the ChangeMessageVisibility API and increase the timeout. Call the GenerateDataKey API for encryption of data. Call the DeleteMessage API to delete the message		-Correct
 Call the DeleteMessage API to increase the timeout.
 Call the ChangeMessageVisibility API and decrease the timeout. Call the DeleteMessage API to delete the message.
 Call the DeleteMessage API to delete the message from the queue first. Call the ChangeMessageVisibility API and increase the timeout
Unattempted
Answer – A
The AWS Documentation mentions the following
Changes the visibility timeout of a specified message in a queue to a new value. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.
For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected.
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dg.pdf
Options B and D are incorrect since you first need to call ChangeMessageVisibility API
Option C is incorrect since the ChangeMessageVisibility API should be used to increase the timeout
For more information on the visibility timeout, please refer to the below URL
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html



34. Question
As a programmer you have been hired to develop an application for a company. The application needs to first encrypt the data at the client side before sending it to a destination location. How can you achieve this? The size of the data is generally around 1 – 4 MB. Each object needs to have its own key to encrypt the data.

 Upload the data to KMS and use the CMK key to encrypt the data
 Use the CMK key ARN to get the key and encrypt the data
 Use the GenerateDataKey API to get the key from a CMK		-Correct
 Upload the data to an S3 bucket with encryption enabled
Unattempted
Answer – A
The AWS Documentation mentions the following
Changes the visibility timeout of a specified message in a queue to a new value. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.
For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected.
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dg.pdf
Options B and D are incorrect since you first need to call ChangeMessageVisibility API
Option C is incorrect since the ChangeMessageVisibility API should be used to increase the timeout
For more information on the visibility timeout, please refer to the below URL
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html



35. Question
As a developer, you have the requirement to access resources in another account. Which of the following is the best way to achieve this?

 Create a cross account role and call the AssumeRole API		-Correct
 Create a user in the destination account and share the password
 Create a user in the destination account and share the Access Keys
 Create an IAM Role and attach it to an EC2 Instance
Unattempted
Answer – A
The AWS Documentation mentions the following
For cross-account access, imagine that you own multiple accounts and need to access resources in each account. You could create long-term credentials in each account to access those resources. However, managing all those credentials and remembering which one can access which account can be time consuming. Instead, you can create one set of long-term credentials in one account and then use temporary security credentials to access all the other accounts by assuming roles in those accounts.
All other options are incorrect since the right option is to use cross account roles
For more information on Assuming a Role, please refer to the below URL
https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html



36. Question
You are a developer for an application. The application needs to make use of AWS for managing authentication. The users should be able to authenticate using identity providers such as Facebook and Google. At the same time, you also need to enable guest user access to limited resources. How can you achieve this in the best possible way?

 Use IAM users and groups
 Use AWS Cognito and identity pools with both authenticated and unauthenticated identities		-Correct
 Use AWS federated identities
 Use AWS Cognito App Sync
Unattempted
Answer – B
The AWS Documentation mentions the following
Amazon Cognito identity pools support both authenticated and unauthenticated identities. Authenticated identities belong to users who are authenticated by any supported identity provider. Unauthenticated identities typically belong to guest users.
Option A is incorrect since this would be too much of a maintenance overhead to maintain the users
Option C is incorrect since we don’t need federation access over here
Option D is incorrect since we don’t need Sync capabilities here
For more information on Identity pools, please refer to the below URL
https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html



37. Question
Your company needs to develop an application that needs to have a caching facility in place. The application cannot afford many cache failures and should be highly available. Which of the following would you choose for this purpose?

 Use Memcached on an EC2 Instance
 Use ElastiCache – Memcached
 Use ElastiCache – Redis in Cluster Mode		-Correct
 Use Redis on an EC2 Instance
Unattempted
Answer – C
The AWS Documentation mentions the following
ElastiCache for Redis has multiple features to enhance reliability for critical production deployments:
Automatic detection and recovery from cache node failures.
Multi-AZ with automatic failover of a failed primary cluster to a read replica in Redis clusters that support replication.
Redis (cluster mode enabled) supports partitioning your data across up to 15 shards.
Redis version 3.2.6 supports in-transit and at-rest encryption with authentication so you can build HIPAA-compliant applications.
Flexible Availability Zone placement of nodes and clusters for increased fault tolerance

Options A and D are incorrect since using just an EC2 Instance would be a single point of failure
Option B is incorrect since Redis would be better for high availability
For more information on Redis ElastiCache, please refer to the below URL
https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html



38. Question
Your team is planning on deploying an application, that processes periodic tasks using the worker environment on AWS Elastic Beanstalk, Which of the following is an additional requirement for a worker environment in AWS Elastic Beanstalk?

 Ensure that the application is uploaded as a zip file
 Ensure that the application size does not exceed 512 MB
 Ensure that the application does not include a parent level folder
 Ensure that the application contains a file called cron.yaml		-Correct
Unattempted
Answer – D
The AWS Documentation mentions the following
When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you’ll need to upload a source bundle. Your source bundle must meet the following requirements:
Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)
Not exceed 512 MB
Not include a parent folder or top-level directory (subdirectories are fine)
If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file
Since this is clearly mentioned in the documentation all other options are incorrect.
For more information on application source bundles in Elastic beanstalk, please refer to the below URL
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html



39. Question
Your team is planning on using the AWS Code Build service to test out the build of the application. The application needs to connect to a database. How should you store the database password in a secure manner so that it is available during the build process?

 Store the password as an environment variable on the build server
 Store the password in AWS Systems Manager Parameter Store		-Correct
 Store the password in a config file on the build server
 Store the password in a config file in the application
Unattempted
Answer – B
The AWS Documentation mentions the following
We strongly discourage using environment variables to store sensitive values, especially AWS access key IDs and secret access keys. Environment variables can be displayed in plain text using tools such as the AWS CodeBuild console and the AWS CLI.
For sensitive values, we recommend you store them in the Amazon EC2 Systems Manager Parameter Store and then retrieve them from your build spec.

All other options are invalid because they are all insecure ways to access passwords in applications from AWS CodeBuild.
For more information on referencing environment variables, please refer to the below URL
https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html



40. Question
Your team has just started using the API gateway service. Several AWS Lambda functions are used as the backend for the gateway service. You have deployed the API and made it available for test users. You have now made a change to the method response for the API gateway. What should you do next?

 Recreate the gateway service
 Redeploy the API		-Correct
 Redeploy the Lambda function
 Make a copy of the gateway API
Unattempted
Answer – B
The AWS Documentation mentions the following
To deploy an API, you create an API deployment and associate it with a stage. Each stage is a snapshot of the API and is made available for the client to call. Every time you update an API, which includes modification of methods, integrations, authorizers, and anything else other than stage settings, you must redeploy the API to an existing stage or to a new stage.
All other options are incorrect since the right way is to Redeploy the API
For more information on how to deploy an API, please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html



41. Question
You are working on a REST API where you require to pass client submitted method requests as it is to a Lambda Function. Which of the following can be set as Integration type for this requirement?

 "type": "aws_proxy"		-Correct
 "type": "aws"
 "type": "http_proxy"
 "type": "mock"
Unattempted
Correct Answer – A
Integration Type “Aws_Proxy” can be used for an API method to be integrated with the Lambda Function where incoming requests from the clients is passed as input to Lambda Function.
Option B is incorrect as with AWS integration , there is a mapping between method request & integration request along with method response & integration response. This is not a suitable integration type for passing client request to Lambda function directly.
Option C is incorrect as “http_proxy” integration allows clients to access backend HTTP endpoints. This is not a suitable integration type for passing client request to Lambda function directly.
Option D is incorrect as mock integration type is used to test integration type with API Gateway sending response without sending request to backend.
For more information on choosing API Gateway API Integration type, refer to the following URL,
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html



42. Question
A company has an application that is making use of a DynamoDB table. There is now a requirement to ensure that all changes to the items in the table are recorded and stored in a MySQL database. Which of the following would ideally be one of the implementation steps?

 Enable DynamoDB Accelerator
 Enable DynamoDB global tables
 Enable DynamoDB streams		-Correct
 Enable DynamoDB triggers
Unattempted
Answer – C
The AWS Documentation mentions the following
DynamoDB Streams enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.
Option A is invalid since this is used to provide DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications
Option B is invalid since this is used to provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution
Option D is invalid since there are no inbuilt triggers in DynamoDB
For more information on DynamoDB streams, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html



43. Question
You have developed a Lambda function. This function needs to run on a scheduled basis. Which of the following can be done to accomplish this requirement in an ideal manner?

 Use the schedule service in AWS Lambda
 Use an EC2 Instance to schedule the Lambda invocation
 Use Cloudwatch events to schedule the Lambda function		-Correct
 Use Cloudtrail to schedule the Lambda function
Unattempted
Answer – C
The AWS Documentation mentions the following
Option A is incorrect since there is no inbuilt scheduler in AWS Lambda
Option B is incorrect since this would add more maintenance overhead
Option D is incorrect since this service is an API monitoring service
For more information on running Lambda functions on schedules, please refer to the below URL
https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html



44. Question
A company has a set of APIs and a web application. They want to deploy it to AWS. They don’t want to manage the underlying infrastructure. Which of the following services can help to accomplish this?

 AWS Lambda and API Gateway		-Correct
 AWS EC2 and Cloudfront
 AWS Lambda and Cloudfront
 AWS Lambda and EC2
Unattempted
Answer – A
The AWS Documentation mentions the following
AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time you consume – there is no charge when your code is not running. With AWS Lambda, you can run code for virtually any type of application or backend service – all with zero administration
Amazon API Gateway is an AWS service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale. You can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.
Options B and D are incorrect since for EC2 you would need to manage the compute layer
Option C is incorrect since Cloudfront is not used along with AWS Lambda
For more information on AWS Lambda and the API gateway service, please refer to the below URL

https://docs.aws.amazon.com/lambda/latest/dg/welcome.html
https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html



45. Question
You are a developer for a company that is planning on using the AWS RDS service. Your Database administrator spins up a new MySQL RDS Instance in AWS. You now need to connect to that instance. How can you achieve this? Choose 2 answers from the options given below.

 Use the DescribeDBInstances API and get the endpoint for the database instance		-Correct
 Use the DescribeDBInstances API and get the IP address for the database instance
 Request an endpoint for the instance from the Database Administrator		-Correct
 Request the IP address of the instance from the Database Administrator
Unattempted
Answer – A and C
The AWS Documentation mentions the following
Before you can connect to a DB instance running the MySQL database engine, you must create a DB instance. For information, seeCreating a DB Instance Running the MySQL Database Engine. Once Amazon RDS provisions your DB instance, you can use any standard MySQL client application or utility to connect to the instance. In the connection string, you specify the DNS address from the DB instance endpoint as the host parameter and specify the port number from the DB instance endpoint as the port parameter.
You can use the AWS Management Console, the AWS CLI describe-db-instances command, or the Amazon RDS API DescribeDBInstances action to list the details of an Amazon RDS DB instance, including its endpoint
Options B and D are incorrect since you need to use the endpoints to connect to the database
For more information on connecting to a database endpoint , please refer to the below URL
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html



46. Question
Your development team is currently working with an application that interacts with the DynamoDB table. Due to the proposed extensive use of the application, the underlying DynamoDB table would undergo a steady growth in size.
Which of the following preferred options should be used for retrieving the data? (Choose 3)

 Use the query operation		-Correct
 Use the Scan operation
 Use the GetItem API command		-Correct
 Use the BatchGetItem API command		-Correct
Unattempted
Answer – A ,C and D
The AWS Documentation mentions the following
If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. (For tables, you can also consider using the GetItem and BatchGetItem APIs.)
Option B is incorrect since this would cause performance issues as the amount of items starts to increase.
For more information on best practises for the query and scan operation, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html



47. Question
Your development team is planning on building an application based on the Microservice architecture pattern. Docker containers would be used to build the application.
Which of the following services should be considered if orchestration is also needed? (Choose 3)

 AWS EC2		-Correct
 AWS ECS		-Correct
 Application Load balancer		-Correct
 Classic Load balancer
Unattempted
Answer – A, B and C
Please refer to the link for “ECS on EC2 instances” on page 126 and 127https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-dg.pdf

So the correct answers are A, B, and C.



48. Question
Your team is looking into the Serverless deployment of an AWS lambda function. The function will be deployed using the Serverless application model. To test this out, you first create a sample function created below.
var AWS = require(‘aws-sdk’);
exports.handler = function(event, context, callback)
{
var bucketName = “Demobucket”;
callback(null, bucketName);
}
What should be the next steps in the serverless deployment? (Choose 2)

 Create a YAML file with the deployment specific’s and package that along with the function file.		-Correct
 Upload the application function file onto an S3 bucket.
 Upload the function on AWS Lambda
 Upload the complete package onto an S3 bucket		-Correct
Unattempted
Answer – A and D
This is given in the AWS Documentation

Option B is incorrect since you need to package both the application function and the YAML file
Option C is incorrect since you have the requirement to deploy this in an automated fashion

Please refer to the below link, Step 3 and Step 4 for further details
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-quick-start.html



49. Question
You’re in charge for creating a cloudformation template. This template needs to create resources for multiple types of environment. The template needs to be flexible so that it can create resources based on the type of environment. How can you achieve this? (Choose 2)

 Create an Input Parameter to take in the type of environment.		-Correct
 Use the Outputs section to define the type of environment
 Use the Custom Resources feature to create resources based on the type of environment
 Use the Conditions section to create resources based on the type of environment		-Correct
Unattempted
Answer – A and D
This is given in the AWS Documentation
The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true. Similarly, you can associate the condition with a property so that AWS CloudFormation only sets the property to a specific value if the condition is true. If the condition is false, AWS CloudFormation sets the property to a different value that you specify.
You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs.
Since this is clearly given in the documentation, all other options are incorrect
For more information on conditions in a Cloudformation template, please refer to the below URL
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html



50. Question
Your team is working on an API definition which will be deployed using the API gateway service. You then need to ensure that control is established on who can access the various resources within the API gateway.
Which of the following can help ensure this security requirement is met? (Choose 3)

 Key Policies
 IAM Policies		-Correct
 Resource Policies		-Correct
 IAM Roles		-Correct
Unattempted
Correct Answer – B, C and D
This is given in the AWS Documentation

For more information on using IAM Policies for controlling access, please refer to the below URL
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html
Resource policies, IAM roles and policies can control access to an API.

https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html



51. Question
Your company has a development application that needs to interact with an S3 bucket. There is a requirement that all data in the bucket is encrypted at rest. You also need to ensure that the keys are managed by you.
Which of the following can you use for this purpose? (Choose 3)

 Server-Side Encryption with AWS Managed Keys
 Server-Side Encryption with AWS KMS Keys		-Correct
 Server-Side Encryption with Customer-Provided Keys		-Correct
 Client-Side Encryption		-Correct
Unattempted
Answer – B, C, and D
This is given in the AWS Documentation
Use Server-Side Encryption with Customer-Provided Keys (SSE-C) – You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects.
You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools
Option B is correct. “Server-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. AWS KMS uses customer master keys(CMKs) to encrypt your Amazon S3 objects.”
https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

Option A is incorrect since here the keys are managed by AWS.
For more information on Server-side encryption for S3, please refer to the below URL
https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html



52. Question
You are developing an application that is going to make use of AWS Cognito. The default sign-in and sign-up features of the AWS Cognito service will be used. There is a security requirement to ensure that if the user’s credentials are compromised, then they would need to use a new password.
Which of the following needs to be in place for this? (Choose 2)

 Ensure to create a user pool in AWS Cognito		-Correct
 Ensure to “Block use” for compromised credentials in the Advanced Security section		-Correct
 Ensure to “Block use” for compromised credentials in the Basic Security section
 Verify sign-in operation on Cognito using Secure Remote Password
Unattempted
Answer – A and B
This is given in the AWS Documentation

Option C is incorrect since this configuration needs to be done in the Advanced Security section

Option D is incorrect as Currently, Amazon Cognito doesn’t check for compromised credentials for sign-in operations with Secure Remote Password (SRP) flow, which doesn’t send the password during sign-in.
For more information on Cognito User pools, please refer to the below URL
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-compromised-credentials.html



53. Question
Your team is currently publishing items to an S3 bucket. You need to record the size of the objects in a separate DynamoDB table.
How can you ensure that each uploaded object triggers a record in the DynamoDB table in an ideal manner? (Choose 2)

 Create a new SQS queue
 Create a new Lambda function		-Correct
 Add the SQS queue to the source event for the S3 bucket
 Add the Lambda function to the source event for the S3 bucket		-Correct
Unattempted
Answer – B and D
This is given in the AWS Documentation
Amazon S3 can publish events (for example, when an object is created in a bucket) to AWS Lambda and invoke your Lambda function by passing the event data as a parameter. This integration enables you to write Lambda functions that process Amazon S3 events. In Amazon S3, you add bucket notification configuration that identifies the type of event that you want Amazon S3 to publish and the Lambda function that you want to invoke.
Options A and C are incorrect since the ideal option would be to create a Lambda function that could be used to automatically record the data size and then place a record in the DynamoDB table
For more information on S3 with Lambda, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html



54. Question
You are working on a system that will make use of AWS Kinesis, and it is getting data from various log sources. You are looking at creating an initial number of shards for the Kinesis stream.
Which of the following can be used in the calculation of initial number of shards for the Kinesis stream? (Choose 2)

 Incoming write bandwidth		-Correct
 Outgoing write bandwidth
 Incoming read bandwidth
 Outgoing read bandwidth		-Correct
Unattempted
Answer – A and D
This is given in the AWS Documentation

For more information on Amazon Kinesis streams, please refer to the below URLhttps://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html



55. Question
Your development team has developed a series of Docker containers that will be part of an application. The deployment team is looking at using the Elastic Container service for hosting these containers.
Which of the following are 2 possible data sources for storing the Docker based images?

 On the EC2 Instances
 In Docker Hub		-Correct
 In the Elastic Container Registry		-Correct
 Store them as Amazon Machine Images
Unattempted
Answer – B and C
The AWS Documentation architecture diagram of the Elastic Container service shows the 2 sources from where you can download the Docker containers

Because the AWS Documentation clearly mentions this , all other options are invalid.

For more information on the Amazon Container Service, please refer to the below URL
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html



56. Question
Your developers have been given access to a CodeCommit Repository. You need to ensure that if any changes are made to a repository, notifications are sent accordingly.
Which of the below 2 destinations can be used for the notifications.

 AWS Lambda		-Correct
 AWS SNS		-Correct
 AWS Config
 AWS IAM
Unattempted
Answer – A and B
The AWS Documentation mentions the following
You can configure an AWS CodeCommit repository so that code pushes or other events trigger actions, such as sending a notification from Amazon Simple Notification Service (Amazon SNS) or invoking a function in AWS Lambda. You can create up to ten triggers for each AWS CodeCommit repository.
Option C is incorrect since this is used to monitor configuration changes
Option D is incorrect since this is used for managing IAM users
For more information on the notification, please refer to the below URL
https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html



57. Question
Your team is working on an application that is going to work with a DynamoDB table. At the design stage, you are trying to find out the optimum way to define partition keys and secondary indexes.
Which of the following are recommendations for defining secondary indexes? (Choose 2)

 Keep the number of indexes to a minimum		-Correct
 Define as many indexes as possible to maximize performance
 Avoid indexing tables that experience heavy write activity		-Correct
 Add indexes to tables that experience heavy write activity
Unattempted
Answer – A and C
The AWS Documentation mentions the following
Keep the number of indexes to a minimum. Don’t create secondary indexes on attributes that you don’t query often. Indexes that are seldom used contribute to increased storage and I/O costs without improving application performance.
Avoid indexing tables that experience heavy write activity. In a data capture application, for example, the cost of I/O operations required to maintain an index on a table with a very high write load can be significant. If you need to index data in such a table, it may be more effective to copy the data to another table that has the necessary indexes and query it there.
Since the documentation mentions this clearly, all other options are invalid
For more information on working with indexes, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html



58. Question
You have to develop an Orders processing system. The system needs to store the Product Information which contains the image for every product.
Which of the following implementation steps should be used when storing the Product related data? (Choose 2)

 Store the Product ID, Name, Price and the path of image stored in S3 bucket in a DynamoDb table		-Correct
 Store the Product Image as an attribute in the same table
 Store the Product ID, Name and price in an S3 bucket
 Store the Product Image in an S3 bucket		-Correct
Unattempted
Answer – A and D
This is also mentioned in the AWS Documentation
As mentioned above, you can also take advantage of Amazon Simple Storage Service (Amazon S3) to store large attribute values that cannot fit in a DynamoDB item. You can store them as an object in Amazon S3 and then store the object identifier in your DynamoDB item.
You can also use the object metadata support in Amazon S3 to provide a link back to the parent item in DynamoDB. Store the primary key value of the item as Amazon S3 metadata of the object in Amazon S3. Doing this often helps with maintenance of the Amazon S3 objects.
Option B is incorrect since the Images should ideally be stored as an object in S3
Option C is incorrect since the ID and Name data should ideally be stored in a table
For more information on best developer practises for DynamoDB, please refer to the below URL
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html



59. Question
Your team is planning on delivering content to users by using the CloudFront service and an S3 bucket as the source. You need to ensure that a custom value is placed for the amount of time the object is stored in the CloudFront cache.
Which 2 of the following options can be used to fulfil this requirement?

 Configure the origin to add an Expires header field to the object		-Correct
 Configure the Cloudfront distribution to add an Expires header field to the object
 Specify a value for Minimum TTL in CloudFront cache behaviors		-Correct
 Specify a value for Minimum TTL in the origin object
Unattempted
Answer – A and C
This is also mentioned in the AWS Documentation
For web distributions, to control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin, you can:
Configure your origin to add a Cache-Control or an Expires header field to each object.
Specify a value for Minimum TTL in CloudFront cache behaviors.
Use the default value of 24 hours.

Since this is clearly mentioned in the AWS Documentation , the other options are invalid
For more information on request and response behaviour for Cloudfront with S3, please refer to the below URL
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorS3Origin.html



60. Question
As a developer you are looking at making use of AWS Cognito Sync.
Which of the below are features of this service? (Choose 3)

 Cross-device syncing of application-related user data		-Correct
 Synching of offline data back to AWS		-Correct
 Push Sync Notification		-Correct
 Sync data with DynamoDB
Unattempted
Answer – A, B and C
The AWS Documentation mentions the following
Amazon Cognito Sync is an AWS service and client library that enable cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and web applications. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.
Since the documentation clearly gives the features of this service, all other options are invalid
For more information on AWS Cognito Sync, please refer to the below URL
https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-cognito-sync.html



61. Question
You’ve developed an application that is going to be hosted on an EC2 Instance. The company has decided to use Cloudfront to distribute the content. The IT Security department has mandated that the traffic is encrypted between Cloudfront and the Viewer and Cloudfront and the origin as well.
How can you achieve this? (Choose 2)

 Ensure that HTTP is mapped to port 443 at the origin
 Ensure that KMS keys are used to encrypt the traffic
 Ensure that the Viewer Protocol policy is set to HTTPS only or Redirect HTTP to HTTPS		-Correct
 Ensure that the Origin Protocol policy is set to HTTPS only		-Correct
Unattempted
Answer – C and D
This is given in the AWS Documentation

Since this is clearly given in the documentation, all other options are incorrect

For more information on configuring HTTPS between the Viewer and Cloudfront and the Origin and Cloudfront, please refer to the below URL

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html



62. Question
Your team is transitioning a stateful based web application to AWS. You need to decide the services which will be used to host the application.
Which of the following would be included in the design? (Choose 2)

 DynamoDB to store the session data		-Correct
 AWS Lambda to store the session data
 An Application Load balancer to distribute traffic		-Correct
 An API gateway to distribute traffic
Unattempted
Answer – A and C
The below example from the AWS Documentation shows how DynamoDB can be used to store session data

And the application load balancer can be used to distribute traffic for the application
Option B is incorrect since this service is used as a Compute Service
Option D is incorrect since this is an API management service
For more information on AWS Cloud Best practises, please refer to the below URL
https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf



63. Question
You are currently working on a function for AWS Lambda. When uploading the package for your AWS Lambda function, you are receiving the following error:
CodeStorageExceededException
What can you do to resolve this error? (Choose 2)

 Raise a call with AWS Support to raise the limit on storage		-Correct
 Change the code to Node.js for taking up less storage
 Reduce the size for your code		-Correct
 Change the memory limit
Unattempted
Answer -A and C
The AWS Documentation mentions the following

Since the documentation clearly mentions this , all other options are invalid.

For more information on limits for AWS Lambda, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/limits.html



64. Question
Your team is developing a series of Lambda functions. You need to ensure that you analyse the invocations of the functions during the testing phase.
Which of the following tools can help you achieve this? (Choose 2)

 Amazon Cloudwatch		-Correct
 Amazon Inspector
 Amazon X-Ray		-Correct
 Amazon Cloudtrail
Unattempted
Answer – A and C
The AWS Documentation mentions the following

Option B is incorrect since this is used to check EC2 Instances for vulnerabilities

Option D is incorrect since this is used to monitor API activity
For more information on troubleshooting AWS Lambda, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/troubleshooting.html



65. Question
Your team is developing a set of Lambda functions. They need to debug the Lambda functions using the X-Ray service.
Which of the following are environment variables which are used by AWS Lambda to communicate with the X-Ray service? (Choose 3)

 _X_AMZN_TRACE_ID		-Correct
 AWS_XRAY_CONTEXT_MISSING		-Correct
 AWS_XRAY_DAEMON_ADDRESS		-Correct
 AWS_LAMBDA_XRAY
Unattempted
Answer – A,B and C
The AWS Documentation mentions the following
AWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK.
_X_AMZN_TRACE_ID: Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID. If Lambda receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment variable. If a tracing header was not received, Lambda will generate one for you.
AWS_XRAY_CONTEXT_MISSING: The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record X-Ray data, but a tracing header is not available. Lambda sets this value to LOG_ERROR by default.
AWS_XRAY_DAEMON_ADDRESS: This environment variable exposes the X-Ray daemon’s address in the following format: IP_ADDRESS:PORT. You can use the X-Ray daemon’s address to send trace data to the X-Ray daemon directly, without using the X-Ray SDK.

For more information on using Lambda with X-Ray, please refer to the below URL
https://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html